{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTETdipRKslE"
   },
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.  \n",
    "All rights reserved.  \n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db18ta9YKslH"
   },
   "source": [
    "### Colab\n",
    "\n",
    "Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/ai-economist/blob/master/tutorials/multi_agent_training_with_rllib.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7u8q3nesKslH"
   },
   "source": [
    "### Prerequisites\n",
    "It is helpful to be familiar with **Foundation**, a multi-agent economic simulator built for the AI Economist ([paper here](https://arxiv.org/abs/2004.13332)). If you haven't worked with Foundation before, we highly recommend taking a look at our other tutorials:\n",
    "\n",
    "- [Foundation: the Basics](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb)\n",
    "- [Extending Foundation](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_advanced.ipynb)\n",
    "- [Optimal Taxation Theory and Simulation](https://github.com/salesforce/ai-economist/blob/master/tutorials/optimal_taxation_theory_and_simulation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGkKf0toKslI"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxskLNk1KslI"
   },
   "source": [
    "Welcome! This tutorial is the first of a series on doing distributed multi-agent reinforcement learning (MARL). Here, we specifically demonstrate how to integrate our multi-agent economic simulation, [Foundation](https://github.com/salesforce/ai-economist/tree/master/ai_economist/foundation), with [RLlib](https://github.com/ray-project/ray/tree/master/rllib), an open-source library for reinforcement learning. We chose to use RLlib, as it provides an easy-to-use and flexible library for MARL. A detailed documentation on RLlib is available [here](https://docs.ray.io/en/master/rllib.html).\n",
    "\n",
    "We put together these tutorial notebook with the following key goals in mind:\n",
    "- Provide an exposition to MARL. While there are many libraries and references out there for single-agent RL training, MARL training is not discussed as much, and there aren't many multi-agent rl libraries.\n",
    "- Provide reference starting code to perform MARL training so the AI Economist community can focus more on building meaningful extensions to Foundation and better-performant algorithms.\n",
    "\n",
    "We will cover the following concepts in this tutorial:\n",
    "1. Adding an *environment wrapper* to make the economic simulation compatible with RLlib.\n",
    "2. Creating a *trainer* object that holds the (multi-agent) policies for environment interaction.\n",
    "3. Training all the agents in the economic simulation.\n",
    "4. Generate a rollout using the trainer object and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4JZv1MbKslJ"
   },
   "source": [
    "### Dependencies:\n",
    "You can install the ai-economist package using \n",
    "- the pip package manager OR\n",
    "- by cloning the ai-economist package and installing the requirements (we shall use this when running on Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T21:06:29.198112Z",
     "start_time": "2023-05-02T21:06:28.792156Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhRHQptlKslK",
    "outputId": "a32e75fa-e086-427f-9ac9-add555aa12cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'ai-economist-fed'\n",
      "/home/supremusdominus/Download/ai-economist-fed/tutorials\n",
      "Requirement already satisfied: pip in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Using cached pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "Successfully installed pip-23.3.1\n",
      "Obtaining file:///home/supremusdominus/Download/ai-economist-fed/tutorials\n",
      "\u001b[31mERROR: file:///home/supremusdominus/Download/ai-economist-fed/tutorials does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from scikit-learn) (1.25.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "import os, signal, sys, time\n",
    "# IN_COLAB = 'google.colab' in sys.modules\n",
    "#\n",
    "# if IN_COLAB:\n",
    "#     !git clone https://github.com/matyascorvinus/ai-economist\n",
    "#\n",
    "#     %cd ai-economist\n",
    "#     !pip install -e .\n",
    "#\n",
    "#     # Restart the Python runtime to automatically use the installed packages\n",
    "#     print(\"\\n\\nRestarting the Python runtime! Please (re-)run the cells below.\")\n",
    "#     time.sleep(1)\n",
    "#     #os.kill(os.getpid(), signal.SIGKILL)\n",
    "# else:\n",
    "#     ! pip install ai-economist\n",
    "# pwd : '/home/supremusdominus/Download/ai-economist/tutorials'\n",
    "# %cd /home/supremusdominus/Download/ai-economist\n",
    "\n",
    "# ! pip install -e .\n",
    "\n",
    "INITIALIZE = True\n",
    "if INITIALIZE:\n",
    "    %cd ai-economist-fed  \n",
    "    !pip install --upgrade pip\n",
    "    !pip install -e . \n",
    "    !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xu9CtjfzKslL"
   },
   "source": [
    "Install OpenAI Gym to help define the environment's observation and action spaces for use with RLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L6nUQInKslM"
   },
   "source": [
    "Install the `RLlib` reinforcement learning library:\n",
    "- First, install TensorFlow\n",
    "- Then, install ray[rllib]\n",
    "\n",
    "Note: RLlib natively supports TensorFlow (including TensorFlow Eager) as well as PyTorch, but most of its internals are framework agnostic. Here's a relevant [blogpost](https://medium.com/distributed-computing-with-ray/lessons-from-implementing-12-deep-rl-algorithms-in-tf-and-pytorch-1b412009297d) that compares running RLlib algorithms with TF and PyTorch. Overall, TF seems to run a bit faster than PyTorch, in our experience, and we will use that in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T21:09:10.275263Z",
     "start_time": "2023-05-02T21:07:21.159635Z"
    },
    "id": "JjbKL75JKslN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym==0.21\r\n",
      "  Using cached gym-0.21.0.tar.gz (1.5 MB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\r\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\r\n",
      "  \r\n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\r\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\r\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[1 lines of output]\u001b[0m\r\n",
      "  \u001b[31m   \u001b[0m error in gym setup command: 'extras_require' must be a dictionary whose values are strings or lists of strings containing valid project/version requirement specifiers.\r\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\r\n",
      "  \r\n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\r\n",
      "\r\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\r\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\r\n",
      "\r\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\r\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.14 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.14\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Collecting ray[rllib]==0.8.4\r\n",
      "  Downloading ray-0.8.4-cp38-cp38-manylinux1_x86_64.whl (20.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting redis>=3.3.2\r\n",
      "  Downloading redis-4.5.4-py3-none-any.whl (238 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting filelock\r\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\r\n",
      "Requirement already satisfied: jsonschema in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from ray[rllib]==0.8.4) (3.2.0)\r\n",
      "Collecting colorama\r\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\r\n",
      "Collecting aiohttp\r\n",
      "  Downloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /home/supremusdominus/.local/lib/python3.8/site-packages (from ray[rllib]==0.8.4) (5.4.1)\r\n",
      "Collecting py-spy>=0.2.0\r\n",
      "  Using cached py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\r\n",
      "Collecting google\r\n",
      "  Using cached google-3.0.0-py2.py3-none-any.whl (45 kB)\r\n",
      "Requirement already satisfied: numpy>=1.16 in /home/supremusdominus/.local/lib/python3.8/site-packages (from ray[rllib]==0.8.4) (1.21.0)\r\n",
      "Collecting grpcio\r\n",
      "  Downloading grpcio-1.54.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: click in /home/supremusdominus/.local/lib/python3.8/site-packages (from ray[rllib]==0.8.4) (8.0.1)\r\n",
      "Collecting protobuf>=3.8.0\r\n",
      "  Downloading protobuf-4.22.3-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tensorboardX\r\n",
      "  Using cached tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\r\n",
      "Requirement already satisfied: scipy in /home/supremusdominus/.local/lib/python3.8/site-packages (from ray[rllib]==0.8.4) (1.6.3)\r\n",
      "Collecting tabulate\r\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\r\n",
      "Requirement already satisfied: pandas in /home/supremusdominus/.local/lib/python3.8/site-packages (from ray[rllib]==0.8.4) (1.2.4)\r\n",
      "Collecting atari-py\r\n",
      "  Downloading atari_py-0.2.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\r\n",
      "\u001b[?25hCollecting dm-tree\r\n",
      "  Downloading dm_tree-0.1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: gym[atari] in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from ray[rllib]==0.8.4) (0.25.2)\r\n",
      "Requirement already satisfied: lz4 in /home/supremusdominus/.local/lib/python3.8/site-packages (from ray[rllib]==0.8.4) (3.1.3)\r\n",
      "Collecting opencv-python-headless\r\n",
      "  Using cached opencv_python_headless-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.2 MB)\r\n",
      "Collecting async-timeout>=4.0.2\r\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\r\n",
      "Collecting aiosignal>=1.1.2\r\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from aiohttp->ray[rllib]==0.8.4) (2.0.4)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/supremusdominus/.local/lib/python3.8/site-packages (from aiohttp->ray[rllib]==0.8.4) (21.2.0)\r\n",
      "Collecting frozenlist>=1.1.1\r\n",
      "  Downloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\r\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\r\n",
      "  Downloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.9/266.9 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: six in /home/supremusdominus/.local/lib/python3.8/site-packages (from atari-py->ray[rllib]==0.8.4) (1.16.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /home/supremusdominus/.local/lib/python3.8/site-packages (from google->ray[rllib]==0.8.4) (4.9.3)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/supremusdominus/.local/lib/python3.8/site-packages (from gym[atari]->ray[rllib]==0.8.4) (6.0.0)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from gym[atari]->ray[rllib]==0.8.4) (2.2.1)\r\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from gym[atari]->ray[rllib]==0.8.4) (0.0.8)\r\n",
      "Collecting ale-py~=0.7.5\r\n",
      "  Downloading ale_py-0.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/supremusdominus/.local/lib/python3.8/site-packages (from jsonschema->ray[rllib]==0.8.4) (67.4.0)\r\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/supremusdominus/.local/lib/python3.8/site-packages (from jsonschema->ray[rllib]==0.8.4) (0.17.3)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/supremusdominus/.local/lib/python3.8/site-packages (from pandas->ray[rllib]==0.8.4) (2021.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/supremusdominus/.local/lib/python3.8/site-packages (from pandas->ray[rllib]==0.8.4) (2.8.1)\r\n",
      "Collecting protobuf>=3.8.0\r\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/supremusdominus/.local/lib/python3.8/site-packages (from tensorboardX->ray[rllib]==0.8.4) (20.9)\r\n",
      "Requirement already satisfied: importlib-resources in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from ale-py~=0.7.5->gym[atari]->ray[rllib]==0.8.4) (5.2.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym[atari]->ray[rllib]==0.8.4) (3.11.0)\r\n",
      "Requirement already satisfied: idna>=2.0 in /home/supremusdominus/.local/lib/python3.8/site-packages (from yarl<2.0,>=1.0->aiohttp->ray[rllib]==0.8.4) (2.10)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/supremusdominus/.local/lib/python3.8/site-packages (from beautifulsoup4->google->ray[rllib]==0.8.4) (2.2.1)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/supremusdominus/.local/lib/python3.8/site-packages (from packaging->tensorboardX->ray[rllib]==0.8.4) (2.4.7)\r\n",
      "Installing collected packages: py-spy, dm-tree, tabulate, protobuf, opencv-python-headless, multidict, grpcio, frozenlist, filelock, colorama, atari-py, async-timeout, yarl, tensorboardX, redis, google, ale-py, aiosignal, aiohttp, ray\r\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 ale-py-0.7.5 async-timeout-4.0.2 atari-py-0.2.9 colorama-0.4.6 dm-tree-0.1.8 filelock-3.12.0 frozenlist-1.3.3 google-3.0.0 grpcio-1.54.0 multidict-6.0.4 opencv-python-headless-4.7.0.72 protobuf-3.20.3 py-spy-0.3.14 ray-0.8.4 redis-4.5.4 tabulate-0.9.0 tensorboardX-2.6 yarl-1.9.2\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Collecting opencv-python\r\n",
      "  Using cached opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/supremusdominus/.local/lib/python3.8/site-packages (from opencv-python) (1.21.0)\r\n",
      "Installing collected packages: opencv-python\r\n",
      "Successfully installed opencv-python-4.7.0.72\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Collecting opencv-python-headless==4.1.2.30\r\n",
      "  Downloading opencv_python_headless-4.1.2.30-cp38-cp38-manylinux1_x86_64.whl (21.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m216.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /home/supremusdominus/.local/lib/python3.8/site-packages (from opencv-python-headless==4.1.2.30) (1.21.0)\r\n",
      "Installing collected packages: opencv-python-headless\r\n",
      "  Attempting uninstall: opencv-python-headless\r\n",
      "    Found existing installation: opencv-python-headless 4.7.0.72\r\n",
      "    Uninstalling opencv-python-headless-4.7.0.72:\r\n",
      "      Successfully uninstalled opencv-python-headless-4.7.0.72\r\n",
      "Successfully installed opencv-python-headless-4.1.2.30\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# # We install these specific versions of tensorflow and rllib, that we used in our work.\n",
    "!pip install gym==0.21\n",
    "!pip install tensorflow==1.14\n",
    "!pip install \"ray[rllib]==0.8.4\"\n",
    "!pip install opencv-python\n",
    "!pip install opencv-python-headless==4.1.2.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T21:06:34.489829Z",
     "start_time": "2023-05-02T21:06:34.331623Z"
    },
    "id": "V_aEzYs7rQbs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rl-warp-drive in /home/supremusdominus/Download/warp-drive (1.6.7)\n",
      "Requirement already satisfied: gym>=0.18 in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from rl-warp-drive) (0.25.2)\n",
      "Requirement already satisfied: matplotlib>=3.2.1 in /home/supremusdominus/.local/lib/python3.8/site-packages (from rl-warp-drive) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /home/supremusdominus/.local/lib/python3.8/site-packages (from rl-warp-drive) (1.21.0)\n",
      "Requirement already satisfied: pycuda==2021.1 in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from rl-warp-drive) (2021.1)\n",
      "Requirement already satisfied: pytest>=6.1.0 in /home/supremusdominus/.local/lib/python3.8/site-packages (from rl-warp-drive) (6.2.4)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /home/supremusdominus/.local/lib/python3.8/site-packages (from rl-warp-drive) (5.4.1)\n",
      "Requirement already satisfied: torch<1.11,>=1.9 in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from rl-warp-drive) (1.10.2)\n",
      "Requirement already satisfied: pytools>=2011.2 in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from pycuda==2021.1->rl-warp-drive) (2022.1.4)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /home/supremusdominus/.local/lib/python3.8/site-packages (from pycuda==2021.1->rl-warp-drive) (1.4.4)\n",
      "Requirement already satisfied: mako in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from pycuda==2021.1->rl-warp-drive) (1.2.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from gym>=0.18->rl-warp-drive) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from gym>=0.18->rl-warp-drive) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/supremusdominus/.local/lib/python3.8/site-packages (from gym>=0.18->rl-warp-drive) (6.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/supremusdominus/.local/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/supremusdominus/.local/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/supremusdominus/.local/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/supremusdominus/.local/lib/python3.8/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (2.8.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/supremusdominus/.local/lib/python3.8/site-packages (from pytest>=6.1.0->rl-warp-drive) (21.2.0)\n",
      "Requirement already satisfied: iniconfig in /home/supremusdominus/.local/lib/python3.8/site-packages (from pytest>=6.1.0->rl-warp-drive) (1.1.1)\n",
      "Requirement already satisfied: packaging in /home/supremusdominus/.local/lib/python3.8/site-packages (from pytest>=6.1.0->rl-warp-drive) (20.9)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in /home/supremusdominus/.local/lib/python3.8/site-packages (from pytest>=6.1.0->rl-warp-drive) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/supremusdominus/.local/lib/python3.8/site-packages (from pytest>=6.1.0->rl-warp-drive) (1.10.0)\n",
      "Requirement already satisfied: toml in /home/supremusdominus/.local/lib/python3.8/site-packages (from pytest>=6.1.0->rl-warp-drive) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from torch<1.11,>=1.9->rl-warp-drive) (3.10.0.0)\n",
      "Requirement already satisfied: six in /home/supremusdominus/.local/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=3.2.1->rl-warp-drive) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym>=0.18->rl-warp-drive) (3.11.0)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /home/supremusdominus/.local/lib/python3.8/site-packages (from pytools>=2011.2->pycuda==2021.1->rl-warp-drive) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/supremusdominus/.local/lib/python3.8/site-packages (from mako->pycuda==2021.1->rl-warp-drive) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install rl-warp-drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T21:06:34.497798Z",
     "start_time": "2023-05-02T21:06:34.492800Z"
    },
    "id": "XXSxlt02KslN"
   },
   "outputs": [],
   "source": [
    "# Change directory to the tutorials folder\n",
    "import os, sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\"/content/ai-economist/tutorials\")\n",
    "else:\n",
    "    os.chdir(os.path.dirname(os.path.abspath(\"__file__\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0ptmCDPKslN"
   },
   "source": [
    "## 1. Adding an Environment Wrapper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56yP4q3zKslN"
   },
   "source": [
    "We first define a configuration (introduced in [the basics tutorial](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basics.ipynb)) for the \"gather-trade-build\" environment with multiple mobile agents (that move, gather resources, build or trade) and a social planner that sets taxes according to (a scaled variant of) the 2018 US tax schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T02:58:26.771229Z",
     "start_time": "2023-05-04T02:58:26.728867Z"
    },
    "id": "_YcfRA8NuAXN"
   },
   "outputs": [],
   "source": [
    "env_config_dict = {\n",
    "    # Scenario name - determines which scenario class to use\n",
    "    \"scenario_name\": \"CovidAndEconomySimulation\",\n",
    "\n",
    "    # The list of components in this simulation\n",
    "    \"components\": [\n",
    "        {\"ControlUSStateOpenCloseStatus\": {\n",
    "            # action cooldown period in days.\n",
    "            # Once a stringency level is set, the state(s) cannot switch to another level\n",
    "            # for a certain number of days (referred to as the \"action_cooldown_period\")\n",
    "            \"action_cooldown_period\": 28\n",
    "        }},\n",
    "        {\"FederalGovernmentSubsidyAndQuantitativePolicies\": {\n",
    "            # The number of subsidy levels.\n",
    "            \"num_subsidy_quantitative_policy_level\": 42,\n",
    "            # The number of days over which the total subsidy amount is evenly rolled out.\n",
    "            \"subsidy_quantitative_policy_interval\": 90,\n",
    "            # The maximum annual subsidy that may be allocated per person.\n",
    "            \"max_annual_monetary_unit_per_person\": 20000,\n",
    "        }},\n",
    "        {\"VaccinationCampaign\": {\n",
    "            # The number of vaccines available per million people everyday.\n",
    "            \"daily_vaccines_per_million_people\": 3000,\n",
    "            # The number of days between vaccine deliveries.\n",
    "            \"delivery_interval\": 1,\n",
    "            # The date (YYYY-MM-DD) when vaccination begins\n",
    "            \"vaccine_delivery_start_date\": \"2021-01-12\",\n",
    "        }},\n",
    "    ],\n",
    "\n",
    "    # Date (YYYY-MM-DD) to start the simulation.\n",
    "    \"start_date\": \"2020-03-22\",\n",
    "    # How long to run the simulation for (in days)\n",
    "    \"episode_length\": 405,\n",
    "\n",
    "    # use_real_world_data (bool): Replay what happened in the real world.\n",
    "    # Real-world data comprises SIR (susceptible/infected/recovered),\n",
    "    # unemployment, government policy, and vaccination numbers.\n",
    "    # This setting also sets use_real_world_policies=True.\n",
    "    \"use_real_world_data\": False,\n",
    "    # use_real_world_policies (bool): Run the environment with real-world policies\n",
    "    # (stringency levels and subsidies). With this setting and\n",
    "    # use_real_world_data=False, SIR and economy dynamics are still\n",
    "    # driven by fitted models.\n",
    "    \"use_real_world_policies\": False,\n",
    "\n",
    "    # A factor indicating how much more the\n",
    "    # states prioritize health (roughly speaking, loss of lives due to\n",
    "    # opening up more) over the economy (roughly speaking, a loss in GDP\n",
    "    # due to shutting down resulting in more unemployment) compared to the\n",
    "    # real-world.\n",
    "    # For example, a value of 1 corresponds to the health weight that\n",
    "    # maximizes social welfare under the real-world policy, while\n",
    "    # a value of 2 means that states care twice as much about public health\n",
    "    # (preventing deaths), while a value of 0.5 means that states care twice\n",
    "    # as much about the economy (preventing GDP drops).\n",
    "    \"health_priority_scaling_agents\": 1,\n",
    "    # Same as above for the planner\n",
    "    \"health_priority_scaling_planner\": 1,\n",
    "\n",
    "    # Full path to the directory containing\n",
    "    # the data, fitted parameters and model constants. This defaults to\n",
    "    # \"ai_economist/datasets/covid19_datasets/data_and_fitted_params\".\n",
    "    # For details on obtaining these parameters, please see the notebook\n",
    "    # \"ai-economist-foundation/ai_economist/datasets/covid19_datasets/\n",
    "    # gather_real_world_data_and_fit_parameters.ipynb\".\n",
    "    \"path_to_data_and_fitted_params\": \"\",\n",
    "\n",
    "    # Economy-related parameters\n",
    "    # Fraction of people infected with COVID-19. Infected people don't work.\n",
    "    \"infection_too_sick_to_work_rate\": 0.1,\n",
    "    # Fraction of the population between ages 18-65.\n",
    "    # This is the subset of the population whose employment/unemployment affects\n",
    "    # economic productivity.\n",
    "    \"pop_between_age_18_65\": 0.6,\n",
    "    # Percentage of interest paid by the federal\n",
    "    # government to borrow money from the federal reserve for COVID-19 relief\n",
    "    # (direct payments). Higher interest rates mean that direct payments\n",
    "    # have a larger cost on the federal government's economic index.\n",
    "    \"risk_free_interest_rate\": 0.03,\n",
    "    # CRRA eta parameter for modeling the economic reward non-linearity.\n",
    "    \"economic_reward_crra_eta\": 2,\n",
    "\n",
    "    # Number of agents in the simulation (50 US states + Washington DC)\n",
    "    \"n_agents\": 51,\n",
    "    # World size: Not relevant to this simulation, but needs to be set for Foundation\n",
    "    \"world_size\": [1, 1],\n",
    "    # Flag to collate all the agents' observations, rewards and done flags into a single matrix\n",
    "    \"collate_agent_step_and_reset_data\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37_c6yeUKslO"
   },
   "source": [
    "Like we have seen in earlier [tutorials](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb), using `env = foundation.make_env_instance(**env_config)` creates an environment instance `env` with the specified configuration.\n",
    "\n",
    "In order to use this environment with RLlib, we will also need to add the environment's `observation_space` and `action_space` attributes. Additionally, the environment itself must subclass the [`MultiAgentEnv`](https://github.com/ray-project/ray/blob/master/rllib/env/multi_agent_env.py) interface, which can return observations and rewards from multiple ready agents per step. To this end, we use an environment [wrapper](https://github.com/salesforce/ai-economist/blob/master/tutorials/rllib/env_wrapper.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T02:58:28.754579Z",
     "start_time": "2023-05-04T02:58:27.019355Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "t73kIcDSKslO",
    "outputId": "fe86c729-7ea8-43e7-e42e-92ff6d0255fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside covid19_components.py: 1 GPUs are available.\n",
      "Inside covid19_env.py: 1 GPUs are available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/supremusdominus/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/home/supremusdominus/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/home/supremusdominus/.local/lib/python3.8/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _nlv = LooseVersion(_np_version)\n",
      "/home/supremusdominus/.local/lib/python3.8/site-packages/setuptools/_distutils/version.py:345: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/home/supremusdominus/.local/lib/python3.8/site-packages/pandas/compat/numpy/function.py:120: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(__version__) >= LooseVersion(\"1.17.0\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the real-world data to only initialize the env, and using the fitted models to step through the env.\n",
      "Loading real-world data from /home/supremusdominus/Download/ai-economist/ai_economist/foundation/scenarios/covid19/../../../datasets/covid19_datasets/data_and_fitted_params\n",
      "Loading fit parameters from /home/supremusdominus/Download/ai-economist/ai_economist/foundation/scenarios/covid19/../../../datasets/covid19_datasets/data_and_fitted_params\n",
      "Using external action inputs.\n"
     ]
    }
   ],
   "source": [
    "from rllib.env_wrapper import FoundationEnvWrapperRlib\n",
    "env_obj = FoundationEnvWrapperRlib({\"env_config_dict\": env_config_dict})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIve5a8XKslP"
   },
   "source": [
    "Upon applying the wrapper to our environment, we have now defined observation and action spaces for the agents and the planner, indicated with `(a)` and `(p)` respectively. Also, (a useful tip) you can still access the environment instance and its attributes simply by using `env_obj.env`\n",
    "\n",
    "In summary, the observation spaces are represented as `Box` objects and the action spaces as `Discrete` objects (for more details on these types, see the OpenAI documentation [page](https://gym.openai.com/docs/#spaces)).\n",
    "\n",
    "Briefly looking at the shapes of the observation features (the numbers in parentheses), you will see that we have some one-dimensional features (e.g. `action-mask`, `flat`, `time`) as well as spatial features (e.g., `world-idx-map`, `world-map`)\n",
    "\n",
    "A couple of quick notes:\n",
    "- An `action_mask` is used to mask out the actions that are not allowed by the environment. For instance, a mobile agent cannot move beyond the boundary of the world. Hence, in position (0, 0), a mobile cannot move \"Left\" or \"Down\", and the corresponding actions in the mask would be nulled out. Now, the RL agent can still recommend to move \"Left\" or \"Down\", but the action isn't really taken.\n",
    "- The key `flat` arises since we set `flatten_observations': True`. Accordingly, the scalar and vector raw observations are all concatenated into this single key. If you're curious to see the entire set of raw observations, do set `flatten_observations': False` in the env_config, and re-run the above cell.\n",
    "\n",
    "Looking at the action spaces, the mobile agents can take 50 possible actions (including 1 NO-OP action or do nothing (always indexed 0), 44 trading-related actions, 4 move actions along the four directions and 1 build action)\n",
    "\n",
    "The planner sets the tax rates for 7 brackets, each from 0-100% in steps of 5%, so that's 21 values. Adding the NO-OP action brings the planner action space to `MultiDiscrete([22 22 22 22 22 22 22])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lIr7PJBKslP"
   },
   "source": [
    "## 2. Creating a *Trainer* Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bjMXlsHKslP"
   },
   "source": [
    "In order to train our economic simulation environment with RLlib, you will need familiarity with one of the key classes: the [`Trainer`](https://docs.ray.io/en/master/rllib-training.html). The trainer object maintains the relationships that connect each agent in the environment to its corresponding trainable policy, and essentially helps in training, checkpointing policies and inferring actions. It helps to co-ordinate the workflow of collecting rollouts and optimizing the various policies via a reinforcement learning algorithm. Inherently, RLlib maintains a wide suite of [algorithms](https://docs.ray.io/en/master/rllib-algorithms.html) for multi-agent learning (which was another strong reason for us to consider using RLlib) - available options include SAC, PPO, PG, A2C, A3C, IMPALA, ES, DDPG, DQN, MARWIL, APEX, and APEX_DDPG. For the remainder of this tutorial, we will stick to using [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/) (PPO), an algorithm known to perform well generally.\n",
    "\n",
    "Every algorithm has a corresponding trainer object; in the context of PPO, we invoke the `PPOTrainer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside covid19_components.py: 1 GPUs are available.\n",
      "Inside covid19_env.py: 1 GPUs are available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/supremusdominus/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n",
      "/home/supremusdominus/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:2804: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the real-world data to only initialize the env, and using the fitted models to step through the env.\n",
      "Loading real-world data from /home/supremusdominus/Download/ai-economist-fed/ai_economist/foundation/scenarios/covid19/../../../datasets/covid19_datasets/data_and_fitted_params\n",
      "Loading fit parameters from /home/supremusdominus/Download/ai-economist-fed/ai_economist/foundation/scenarios/covid19/../../../datasets/covid19_datasets/data_and_fitted_params\n",
      "Loading HDI data from /home/supremusdominus/Download/ai-economist-fed/ai_economist/foundation/scenarios/covid19/../../../datasets/covid19_datasets/data_and_fitted_params/US-Final-human-development-index-learner-OurWorldInData.csv\n",
      "Using external action inputs.\n",
      "Number of US states: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 08:51:41,283\tINFO resource_spec.py:204 -- Starting Ray with 1.76 GiB memory available for workers and up to 0.9 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2023-10-30 08:51:42,297\tINFO services.py:1146 -- View the Ray dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "/home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages/ray/node.py:511: DeprecationWarning: Redis.hmset() is deprecated. Use Redis.hset() instead.\n",
      "  redis_client.hmset(\"webui\", {\"url\": self._webui_url})\n",
      "/home/supremusdominus/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages/ray/worker.py:1207: DeprecationWarning: Redis.hmset() is deprecated. Use Redis.hset() instead.\n",
      "  worker.redis_client.hmset(b\"Drivers:\" + worker.worker_id, driver_info)\n",
      "2023-10-30 08:51:43,495\tINFO trainer.py:583 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "policy tuple value 2 (action_space) must be a gym.Space, got <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e6dc1637b63e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;31m# Create the PPO trainer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m trainer = PPOTrainer(\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFoundationEnvWrapperRlib\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mlogger_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_logger_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, logger_creator)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m_setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mget_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0;31m# Evaluation setup.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m    113\u001b[0m                                             config)\n\u001b[1;32m    114\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 self.workers = self._make_workers(env_creator, self._policy,\n\u001b[0m\u001b[1;32m    116\u001b[0m                                                   \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                                                   self.config[\"num_workers\"])\n",
      "\u001b[0;32m~/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m_make_workers\u001b[0;34m(self, env_creator, policy, config, num_workers)\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0mWorkerSet\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mWorkerSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \"\"\"\n\u001b[0;32m--> 662\u001b[0;31m         return WorkerSet(\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0menv_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_creator, policy, trainer_config, num_workers, logdir, _setup)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# Always create a local worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             self._local_worker = self._make_worker(\n\u001b[0m\u001b[1;32m     62\u001b[0m                 RolloutWorker, env_creator, policy, 0, self._local_config)\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m_make_worker\u001b[0;34m(self, cls, env_creator, policy, worker_index, config)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"multiagent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policies\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"multiagent\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"policies\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0m_validate_multiagent_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_none_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SupremusDominusDominatus/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\u001b[0m in \u001b[0;36m_validate_multiagent_config\u001b[0;34m(policy, allow_none_graph)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \"gym.Space, got {}\".format(type(v[1])))\n\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             raise ValueError(\"policy tuple value 2 (action_space) must be a \"\n\u001b[0m\u001b[1;32m    921\u001b[0m                              \"gym.Space, got {}\".format(type(v[2])))\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: policy tuple value 2 (action_space) must be a gym.Space, got <class 'int'>"
     ]
    }
   ],
   "source": [
    "env_config_dict = {\n",
    "    # Scenario name - determines which scenario class to use\n",
    "    \"scenario_name\": \"CovidAndEconomySimulation\",\n",
    "\n",
    "    # The list of components in this simulation\n",
    "    \"components\": [\n",
    "        {\"ControlUSStateOpenCloseStatus\": {\n",
    "            # action cooldown period in days.\n",
    "            # Once a stringency level is set, the state(s) cannot switch to another level\n",
    "            # for a certain number of days (referred to as the \"action_cooldown_period\")\n",
    "            \"action_cooldown_period\": 28\n",
    "        }},\n",
    "        {\"FederalGovernmentSubsidyAndQuantitativePolicies\": {\n",
    "            # The number of subsidy levels.\n",
    "            \"num_subsidy_quantitative_policy_level\": 141,\n",
    "            # The number of days over which the total subsidy amount is evenly rolled out.\n",
    "            \"subsidy_quantitative_policy_interval\": 90,\n",
    "            # The maximum annual subsidy that may be allocated per person.\n",
    "            \"max_annual_monetary_unit_per_person\": 20000,\n",
    "        }},\n",
    "        {\"VaccinationCampaign\": {\n",
    "            # The number of vaccines available per million people everyday.\n",
    "            \"daily_vaccines_per_million_people\": 3000,\n",
    "            # The number of days between vaccine deliveries.\n",
    "            \"delivery_interval\": 1,\n",
    "            # The date (YYYY-MM-DD) when vaccination begins\n",
    "            \"vaccine_delivery_start_date\": \"2021-01-12\",\n",
    "        }},\n",
    "    ],\n",
    "\n",
    "    # Date (YYYY-MM-DD) to start the simulation.\n",
    "    \"start_date\": \"2020-03-22\",\n",
    "    # How long to run the simulation for (in days)\n",
    "    \"episode_length\": 600,\n",
    "\n",
    "    # use_real_world_data (bool): Replay what happened in the real world.\n",
    "    # Real-world data comprises SIR (susceptible/infected/recovered),\n",
    "    # unemployment, government policy, and vaccination numbers.\n",
    "    # This setting also sets use_real_world_policies=True.\n",
    "    \"use_real_world_data\": False,\n",
    "    # use_real_world_policies (bool): Run the environment with real-world policies\n",
    "    # (stringency levels and subsidies). With this setting and\n",
    "    # use_real_world_data=False, SIR and economy dynamics are still\n",
    "    # driven by fitted models.\n",
    "    \"use_real_world_policies\": False,\n",
    "\n",
    "    # A factor indicating how much more the\n",
    "    # states prioritize health (roughly speaking, loss of lives due to\n",
    "    # opening up more) over the economy (roughly speaking, a loss in GDP\n",
    "    # due to shutting down resulting in more unemployment) compared to the\n",
    "    # real-world.\n",
    "    # For example, a value of 1 corresponds to the health weight that\n",
    "    # maximizes social welfare under the real-world policy, while\n",
    "    # a value of 2 means that states care twice as much about public health\n",
    "    # (preventing deaths), while a value of 0.5 means that states care twice\n",
    "    # as much about the economy (preventing GDP drops).\n",
    "    \"health_priority_scaling_agents\": 1,\n",
    "    # Same as above for the planner\n",
    "    \"health_priority_scaling_planner\": 1,\n",
    "\n",
    "    # Full path to the directory containing\n",
    "    # the data, fitted parameters and model constants. This defaults to\n",
    "    # \"ai_economist/datasets/covid19_datasets/data_and_fitted_params\".\n",
    "    # For details on obtaining these parameters, please see the notebook\n",
    "    # \"ai-economist-foundation/ai_economist/datasets/covid19_datasets/\n",
    "    # gather_real_world_data_and_fit_parameters.ipynb\".\n",
    "    \"path_to_data_and_fitted_params\": \"\",\n",
    "\n",
    "    # Economy-related parameters\n",
    "    # Fraction of people infected with COVID-19. Infected people don't work.\n",
    "    \"infection_too_sick_to_work_rate\": 0.1,\n",
    "    # Fraction of the population between ages 18-65.\n",
    "    # This is the subset of the population whose employment/unemployment affects\n",
    "    # economic productivity.\n",
    "    \"pop_between_age_18_65\": 0.6,\n",
    "    # Percentage of interest paid by the federal\n",
    "    # government to borrow money from the federal reserve for COVID-19 relief\n",
    "    # (direct payments). Higher interest rates mean that direct payments\n",
    "    # have a larger cost on the federal government's economic index.\n",
    "    \"risk_free_interest_rate\": 0.03,\n",
    "    # CRRA eta parameter for modeling the economic reward non-linearity.\n",
    "    \"economic_reward_crra_eta\": 2,\n",
    "\n",
    "    # Number of agents in the simulation (50 US states + Washington DC)\n",
    "    \"n_agents\": 51,\n",
    "    # World size: Not relevant to this simulation, but needs to be set for Foundation\n",
    "    \"world_size\": [1, 1],\n",
    "    # Flag to collate all the agents' observations, rewards and done flags into a single matrix\n",
    "    \"collate_agent_step_and_reset_data\": True,\n",
    "}\n",
    "from rllib.env_wrapper import FoundationEnvWrapperRlib\n",
    "env_obj = FoundationEnvWrapperRlib({\"env_config_dict\": env_config_dict})\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "policies = {\n",
    "    \"a\": (\n",
    "        None,  # uses default policy\n",
    "        env_obj.observation_space,\n",
    "        env_obj.action_space,\n",
    "        {}  # define a custom agent policy configuration.\n",
    "    ),\n",
    "    \"p\": (\n",
    "        None,  # uses default policy\n",
    "        env_obj.observation_space,\n",
    "        env_obj.action_space_pl,\n",
    "        {}  # define a custom planner policy configuration.\n",
    "    )\n",
    "}\n",
    "\n",
    "# In foundation, all the agents have integer ids and the social planner has an id of \"p\"\n",
    "policy_mapping_fun = lambda i: \"a\" if str(i).isdigit() else \"p\"\n",
    "\n",
    "policies_to_train = [\"a\", \"p\"]\n",
    "\n",
    "trainer_config = {\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policies_to_train\": policies_to_train,\n",
    "        \"policy_mapping_fn\": policy_mapping_fun,\n",
    "    }\n",
    "}\n",
    "\n",
    "trainer_config.update(\n",
    "    {\n",
    "        \"num_workers\": 2,\n",
    "        \"num_envs_per_worker\": 2,\n",
    "        # Other training parameters\n",
    "        \"train_batch_size\":  4000,\n",
    "        \"sgd_minibatch_size\": 4000,\n",
    "        \"num_sgd_iter\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "# We also add the \"num_envs_per_worker\" parameter for the env. wrapper to index the environments.\n",
    "env_config = {\n",
    "    \"env_config_dict\": env_config_dict,\n",
    "    \"num_envs_per_worker\": trainer_config.get('num_envs_per_worker'),\n",
    "}\n",
    "\n",
    "trainer_config.update(\n",
    "    {\n",
    "        \"env_config\": env_config\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(webui_host=\"127.0.0.1\")\n",
    "\n",
    "# Create the PPO trainer.\n",
    "trainer = PPOTrainer(\n",
    "    env=FoundationEnvWrapperRlib,\n",
    "    config=trainer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv8bKQIkKslP"
   },
   "source": [
    "PPOTrainer can be instantiated with \n",
    "- `env`: an environment creator (i.e, RLlibEnvWrapper() in our case)\n",
    "- `config`: algorithm-specific configuration data for setting the various components of the RL training loop including the environment, rollout worker processes, training resources, degree of parallelism, framework used, and the policy exploration strategies.\n",
    "\n",
    "Note: There are several configuration settings related to policy architectures, rollout collection, minibatching, and other important hyperparameters, that need to be set carefully in order to train effectively. For the sake of the high-level exposition, we allow RLlib to use most of the the default settings. Check out the list of default [common configuration parameters](https://docs.ray.io/en/releases-0.8.4/rllib-training.html#common-parameters) and default [PPO-specific configuration parameters](https://docs.ray.io/en/releases-0.8.4/rllib-algorithms.html?highlight=PPO#proximal-policy-optimization-ppo). Custom environment configurations may be passed to environment creator via `config[\"env_config\"]`.\n",
    "\n",
    "RLlib also chooses default built-in [models](https://docs.ray.io/en/releases-0.8.4/rllib-models.html#built-in-models-and-preprocessors) for processing the observations. The models are picked based on a simple heuristic: a [vision](https://github.com/ray-project/ray/blob/master/rllib/models/tf/visionnet.py) network for observations that have shape of length larger than 2 (for example, (84 x 84 x 3)), and a [fully connected](https://github.com/ray-project/ray/blob/master/rllib/models/tf/fcnet.py) network for everything else. Custom models can be configured via the `config[\"policy\"][\"model\"]` key.\n",
    "\n",
    "In the context of multi-agent training, we will also need to set the multi-agent configuration:\n",
    "```python\n",
    "\"multiagent\": {\n",
    "        # Map of type MultiAgentPolicyConfigDict from policy ids to tuples\n",
    "        # of (policy_cls, obs_space, act_space, config). This defines the\n",
    "        # observation and action spaces of the policies and any extra config.\n",
    "        \"policies\": {},\n",
    "        # Function mapping agent ids to policy ids.\n",
    "        \"policy_mapping_fn\": None,\n",
    "        # Optional list of policies to train, or None for all policies.\n",
    "        \"policies_to_train\": None,\n",
    "    },\n",
    "```\n",
    "\n",
    "To this end, let's notate the agent policy id by `\"a\"` and the planner policy id by `\"p\"`. We can set `policies`, `policy_mapping_fun` and `policies_to_train` as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To2aBqVkKslQ"
   },
   "source": [
    "Create a multiagent trainer config holding the trainable policies and their mappings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlXDzKXVKslQ"
   },
   "source": [
    "With distributed RL, architectures typically comprise several **roll-out** and **trainer** workers operating in tandem\n",
    "![](assets/distributed_rl_architecture.png)\n",
    "\n",
    "The roll-out workers repeatedly step through the environment to generate and collect roll-outs in parallel, using the actions sampled from the policy models on the roll-out workers or provided by the trainer worker.\n",
    "Roll-out workers typically use CPU machines, and sometimes, GPU machines for richer environments.\n",
    "Trainer workers gather the roll-out data (asynchronously) from the roll-out workers and optimize policies on CPU or GPU machines.\n",
    "\n",
    "In this context, we can also add a `num_workers` configuration parameter to specify the number of rollout workers, i.e, those responsible for gathering rollouts. Note: setting `num_workers=0` will mean the rollouts will be collected by the trainer worker itself. Also, each worker can collect rollouts from multiple environments in parallel, which is specified in `num_envs_per_worker`; there will be a total of `num_workers` $\\times$ `num_envs_per_worker` environment replicas used to gather rollouts.\n",
    "Note: below, we also update some of the default trainer settings to keep the iteration time small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ymw2yiAQKslR"
   },
   "source": [
    "Finally, we need to add the environment configuration to the trainer configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTToecbKKslR"
   },
   "source": [
    "One the training configuration is set, we will need to initialize ray and create the PPOTrainer object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvsQg49FKslR"
   },
   "source": [
    "## 3. Perform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmCNTKcDKslR"
   },
   "source": [
    "And that's it! We are now ready to perform training by invoking `trainer.train()`; we call it for just a few number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Rosuv0AtKslR",
    "outputId": "63a068b5-fb3a-4a8a-f111-f176374fe632",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_ITERS = 100 \n",
    "for iteration in range(NUM_ITERS):\n",
    "    print(f'********** Iter : {iteration} **********')\n",
    "    result = trainer.train()\n",
    "    print(f'''episode_reward_mean: {result.get('episode_reward_mean')}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HctwigPSKslR"
   },
   "source": [
    "By default, the results will be logged to a subdirectory of `~/ray_results`. This subdirectory will contain a file `params.json` which contains the hyperparameters, a file `result.json` which contains a training summary for each episode and a TensorBoard file that can be used to visualize training process with TensorBoard by running|\n",
    "```shell\n",
    "tensorboard --logdir ~/ray_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YInHZMtYKslS"
   },
   "source": [
    "## 4. Generate and Visualize the Environment's Dense Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8hDNhyeKslS"
   },
   "source": [
    "At any point during training, we would also want to inspect the environment's dense logs in order to deep-dive into the training results. Introduced in our [basic tutorial](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb#Visualize-using-dense-logging), dense logs are basically logs of each agent's states, actions and rewards at every point in time, along with a snapshot of the world state.\n",
    "\n",
    "There are two equivalent ways to fetch the environment's dense logs using the trainer object.\n",
    "\n",
    "a. Simply retrieve the dense log from the workers' environment objects\n",
    "\n",
    "b. Generate dense log(s) from the most recent trainer policy model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7nf_Hy0KslS"
   },
   "source": [
    "### 4a. Simply retrieve the dense log from the workers' environment objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbYDzVkvKslS"
   },
   "source": [
    "From each rollout worker, it's straightforward to retrieve the dense logs using some of the function attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOPkYoToKslS"
   },
   "outputs": [],
   "source": [
    "# Below, we fetch the dense logs for each rollout worker and environment within\n",
    "\n",
    "dense_logs = {}\n",
    "# Note: worker 0 is reserved for the trainer actor\n",
    "for worker in range((trainer_config[\"num_workers\"] > 0), trainer_config[\"num_workers\"] + 1):\n",
    "    for env_id in range(trainer_config[\"num_envs_per_worker\"]):\n",
    "        dense_logs[\"worker={};env_id={}\".format(worker, env_id)] = \\\n",
    "        trainer.workers.foreach_worker(lambda w: w.async_env)[worker].envs[env_id].env.previous_episode_dense_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy5bGmvPKslS"
   },
   "outputs": [],
   "source": [
    "# We should have num_workers x num_envs_per_worker number of dense logs\n",
    "print(dense_logs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zm0S6QneKslS"
   },
   "source": [
    "### 4b. Generate a dense log from the most recent trainer policy model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbILmlVrKslS"
   },
   "source": [
    "We may also use the trainer object directly to play out an episode. The advantage of this approach is that we can re-sample the policy model any number of times and generate several rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2oIv-StKslS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from utils import plotting  # plotting utilities for visualizing env. state\n",
    "\n",
    "def do_plot(env, ax, fig):\n",
    "    \"\"\"Plots world state during episode sampling.\"\"\"\n",
    "    maps = env.env.world.maps\n",
    "    locs = [agent.loc for agent in env.env.world.agents] \n",
    "\n",
    "    cmap_order = None\n",
    "    plotting.plot_map(maps, locs, ax, cmap_order)\n",
    "    ax.set_aspect('equal')\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "def generate_rollout_from_current_trainer_policy(\n",
    "    trainer, \n",
    "    env_obj,\n",
    "    num_dense_logs=1\n",
    "):\n",
    "    dense_logs = {}\n",
    "    for idx in range(num_dense_logs):\n",
    "        # Set initial states\n",
    "        agent_states = {}\n",
    "        for agent_idx in range(env_obj.env.n_agents):\n",
    "            agent_states[str(agent_idx)] = trainer.get_policy(\"a\").get_initial_state()\n",
    "        planner_states = trainer.get_policy(\"p\").get_initial_state()   \n",
    "\n",
    "        # Play out the episode\n",
    "        obs = env_obj.reset(force_dense_logging=True)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        for t in range(env_obj.env.episode_length):\n",
    "            actions = {}\n",
    "            for agent_idx in range(env_obj.env.n_agents):\n",
    "                # Use the trainer object directly to sample actions for each agent\n",
    "                actions[str(agent_idx)] = trainer.compute_action(\n",
    "                    obs[str(agent_idx)], \n",
    "                    agent_states[str(agent_idx)], \n",
    "                    policy_id=\"a\",\n",
    "                    full_fetch=False\n",
    "                )\n",
    "\n",
    "            # Action sampling for the planner\n",
    "            actions[\"p\"] = trainer.compute_action(\n",
    "                obs['p'], \n",
    "                planner_states, \n",
    "                policy_id='p',\n",
    "                full_fetch=False\n",
    "            )\n",
    "\n",
    "            obs, rew, done, info = env_obj.step(actions)        \n",
    "            if ((t+1) % 100) == 0:\n",
    "                do_plot(env_obj, ax, fig)\n",
    "            if done['__all__']:\n",
    "                break\n",
    "        dense_logs[idx] = env_obj.env.dense_log\n",
    "    return dense_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wj10lcDVKslT"
   },
   "outputs": [],
   "source": [
    "dense_logs = generate_rollout_from_current_trainer_policy(\n",
    "    trainer, \n",
    "    env_obj,\n",
    "    num_dense_logs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRfC7ap8KslT"
   },
   "source": [
    "### Visualizing the episode dense logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJiAqD5NKslT"
   },
   "source": [
    "Once we obtain the dense logs, we can use the plotting utilities we have created to examine the episode dense logs and visualize the the world state, agent-wise quantities, movement, and trading events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLbq2m3jKslT"
   },
   "outputs": [],
   "source": [
    "from utils import plotting  # plotting utilities for visualizing env. state\n",
    "\n",
    "dense_log_idx = 0\n",
    "for index, element in enumerate(dense_logs):\n",
    "  print('Plot ', index)\n",
    "  plotting.breakdown(dense_logs[index]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CgYa-IlKslT"
   },
   "outputs": [],
   "source": [
    "# Shutdown Ray after use\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzYAX5FYKslT"
   },
   "source": [
    "And that's it for now. See you in the [next](https://github.com/salesforce/ai-economist/blob/master/tutorials/two_level_curriculum_learning_with_rllib.md) tutorial :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "9dbfbadc71e42abc9cd558c210cdff0e47003e6a6f6a3b108a42ec1174a9608f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
