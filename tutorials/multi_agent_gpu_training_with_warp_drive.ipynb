{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1349103",
   "metadata": {},
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.  \n",
    "All rights reserved.\n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f928d76",
   "metadata": {},
   "source": [
    "### Colab\n",
    "\n",
    "Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/ai-economist/blob/master/tutorials/multi_agent_gpu_training_with_warp_drive.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d85d358",
   "metadata": {},
   "source": [
    "# ⚠️ PLEASE NOTE:\n",
    "This notebook runs on a GPU runtime.\\\n",
    "If running on Colab, choose Runtime > Change runtime type from the menu, then select `GPU` in the 'Hardware accelerator' dropdown menu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073387a4",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome! In this tutorial, we detail how we train multi-agent economic simulations built using [Foundation](https://github.com/salesforce/ai-economist/tree/master/ai_economist/foundation) and train it using [WarpDrive](https://github.com/salesforce/warp-drive), an open-source library we built for extremely fast multi-agent reinforcement learning (MARL) on a single GPU. For the purposes of exposition, we specifically consider the [COVID-19 and economy simulation](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/scenarios/covid19/covid19_env.py). The COVID-19 and economy is a simulation to model health and economy dynamics amidst the COVID-19 pandemic and comprises 52 agents.\n",
    "\n",
    "We put together this tutorial with these goals in mind:\n",
    "- Describe how we train multi-agent simulations from scratch, starting with just a Python implementation of the environment on a CPU.\n",
    "- Provide reference starting code to help perform extremely fast MARL training so the AI Economist community can focus more towards contributing multi-agent simulations to Foundation.\n",
    "\n",
    "We will cover the following concepts:\n",
    "1. Building a GPU-compatible environment.\n",
    "2. CPU-GPU environment consistency checker.\n",
    "3. Adding an *environment wrapper*.\n",
    "4. Creating a *trainer* object, and perform training.\n",
    "5. Generate a rollout using the trainer object and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e761c0d",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "It is helpful to be familiar with [Foundation](https://github.com/salesforce/ai-economist/tree/master/ai_economist/foundation), a multi-agent economic simulator, and also the COVID-19 and Economic simulation ([paper here](https://arxiv.org/abs/2108.02904)). We recommend taking a look at the following tutorials:\n",
    "\n",
    "- [Foundation: the Basics](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb)\n",
    "- [Extending Foundation](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_advanced.ipynb)\n",
    "- [COVID-19 and Economic Simulation](https://github.com/salesforce/ai-economist/blob/master/tutorials/covid19_and_economic_simulation.ipynb)\n",
    "\n",
    "It is also important to get familiarized with [WarpDrive](https://github.com/salesforce/warp-drive), a framework we developed for extremely fast end-to-end reinforcement learning on a single GPU. We also have a detailed tutorial on on how to [create custom environments](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md) and integrate with WarpDrive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87df34",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bb9c3b",
   "metadata": {},
   "source": [
    "You will need to install the [AI Economist](https://github.com/salesforce/ai-economist) and [WarpDrive](https://github.com/salesforce/warp-drive) pip packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, signal, sys, time\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/salesforce/ai-economist.git\n",
    "\n",
    "    %cd ai-economist\n",
    "    !pip install -e .\n",
    "    \n",
    "    # Restart the Python runtime to automatically use the installed packages\n",
    "    print(\"\\n\\nRestarting the Python runtime! Please (re-)run the cells below.\")\n",
    "    time.sleep(1)\n",
    "    os.kill(os.getpid(), signal.SIGKILL)\n",
    "else:\n",
    "    !pip install ai-economist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a22701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that a GPU is present.\n",
    "import GPUtil\n",
    "num_gpus_available = len(GPUtil.getAvailable())\n",
    "assert num_gpus_available > 0, \"This notebook needs a GPU machine to run!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a36cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rl-warp-drive\n",
      "  Downloading rl_warp_drive-2.1.0-py3-none-any.whl (381 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.7/381.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytest>=6.1.0 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from rl-warp-drive) (7.2.0)\n",
      "Requirement already satisfied: gym<0.26,>=0.18 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from rl-warp-drive) (0.21.0)\n",
      "Requirement already satisfied: pycuda==2022.1 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from rl-warp-drive) (2022.1)\n",
      "Collecting numba>=0.54.0\n",
      "  Downloading numba-0.56.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.1 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from rl-warp-drive) (3.6.1)\n",
      "Collecting rl-warp-drive\n",
      "  Using cached rl_warp_drive-2.0.2-py3-none-any.whl (381 kB)\n",
      "  Using cached rl_warp_drive-2.0.1-py3-none-any.whl (375 kB)\n",
      "  Using cached rl_warp_drive-2.0-py3-none-any.whl (375 kB)\n",
      "  Using cached rl_warp_drive-1.7.0-py3-none-any.whl (336 kB)\n",
      "  Using cached rl_warp_drive-1.6.7-py3-none-any.whl (336 kB)\n",
      "Collecting pycuda==2021.1\n",
      "  Using cached pycuda-2021.1.tar.gz (1.7 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rl-warp-drive\n",
      "  Using cached rl_warp_drive-1.6.5-py3-none-any.whl (323 kB)\n",
      "  Using cached rl_warp_drive-1.6.4-py3-none-any.whl (121 kB)\n",
      "  Using cached rl_warp_drive-1.6.3-py3-none-any.whl (121 kB)\n",
      "  Using cached rl_warp_drive-1.6.2-py3-none-any.whl (121 kB)\n",
      "  Using cached rl_warp_drive-1.6.1-py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from rl-warp-drive) (1.23.4)\n",
      "Collecting torch>=1.9.0\n",
      "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m866.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.4 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from rl-warp-drive) (6.0)\n",
      "Requirement already satisfied: mako in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from pycuda==2021.1->rl-warp-drive) (1.2.3)\n",
      "Requirement already satisfied: pytools>=2011.2 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from pycuda==2021.1->rl-warp-drive) (2022.1.12)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from pycuda==2021.1->rl-warp-drive) (1.4.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from gym<0.26,>=0.18->rl-warp-drive) (2.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (21.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (9.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (2.8.2)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from pytest>=6.1.0->rl-warp-drive) (2.0.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from pytest>=6.1.0->rl-warp-drive) (1.0.0)\n",
      "Requirement already satisfied: iniconfig in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from pytest>=6.1.0->rl-warp-drive) (1.1.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from pytest>=6.1.0->rl-warp-drive) (22.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from pytest>=6.1.0->rl-warp-drive) (1.0.0rc9)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from torch>=1.9.0->rl-warp-drive) (4.4.0)\n",
      "Requirement already satisfied: wheel in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9.0->rl-warp-drive) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9.0->rl-warp-drive) (61.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.2.1->rl-warp-drive) (1.16.0)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from pytools>=2011.2->pycuda==2021.1->rl-warp-drive) (2.5.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/supremusdominus/anaconda3/envs/ML/lib/python3.10/site-packages (from mako->pycuda==2021.1->rl-warp-drive) (2.1.1)\n",
      "Building wheels for collected packages: pycuda\n",
      "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pycuda \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[134 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m *************************************************************\n",
      "  \u001b[31m   \u001b[0m *** I have detected that you have not run configure.py.\n",
      "  \u001b[31m   \u001b[0m *************************************************************\n",
      "  \u001b[31m   \u001b[0m *** Additionally, no global config files were found.\n",
      "  \u001b[31m   \u001b[0m *** I will go ahead with the default configuration.\n",
      "  \u001b[31m   \u001b[0m *** In all likelihood, this will not work out.\n",
      "  \u001b[31m   \u001b[0m ***\n",
      "  \u001b[31m   \u001b[0m *** See README_SETUP.txt for more information.\n",
      "  \u001b[31m   \u001b[0m ***\n",
      "  \u001b[31m   \u001b[0m *** If the build does fail, just re-run configure.py with the\n",
      "  \u001b[31m   \u001b[0m *** correct arguments, and then retry. Good luck!\n",
      "  \u001b[31m   \u001b[0m *************************************************************\n",
      "  \u001b[31m   \u001b[0m *** HIT Ctrl-C NOW IF THIS IS NOT WHAT YOU WANT\n",
      "  \u001b[31m   \u001b[0m *************************************************************\n",
      "  \u001b[31m   \u001b[0m Continuing in 10 seconds...\n",
      "  \u001b[31m   \u001b[0m Continuing in 9 seconds...\n",
      "  \u001b[31m   \u001b[0m Continuing in 8 seconds...\n",
      "  \u001b[31m   \u001b[0m Continuing in 7 seconds...\n",
      "  \u001b[31m   \u001b[0m Continuing in 6 seconds...\n",
      "  \u001b[31m   \u001b[0m Continuing in 5 seconds...\n",
      "  \u001b[31m   \u001b[0m Continuing in 4 seconds...\n",
      "  \u001b[31m   \u001b[0m Continuing in 3 seconds...\n",
      "  \u001b[31m   \u001b[0m Continuing in 2 seconds...\n",
      "  \u001b[31m   \u001b[0m Continuing in 1 seconds...\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-build-env-dlt49o47/overlay/lib/python3.10/site-packages/setuptools/_distutils/dist.py:264: UserWarning: Unknown distribution option: 'test_requires'\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg)\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/cumath.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/_mymako.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/debug.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/curandom.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/autoinit.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/reduction.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/autoprimaryctx.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/characterize.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/_cluda.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/driver.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/elementwise.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/tools.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/gpuarray.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/__init__.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/scan.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/compiler.py -> build/lib.linux-x86_64-cpython-310/pycuda\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/pycuda/gl\n",
      "  \u001b[31m   \u001b[0m copying pycuda/gl/autoinit.py -> build/lib.linux-x86_64-cpython-310/pycuda/gl\n",
      "  \u001b[31m   \u001b[0m copying pycuda/gl/__init__.py -> build/lib.linux-x86_64-cpython-310/pycuda/gl\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/pycuda/sparse\n",
      "  \u001b[31m   \u001b[0m copying pycuda/sparse/inner.py -> build/lib.linux-x86_64-cpython-310/pycuda/sparse\n",
      "  \u001b[31m   \u001b[0m copying pycuda/sparse/pkt_build.py -> build/lib.linux-x86_64-cpython-310/pycuda/sparse\n",
      "  \u001b[31m   \u001b[0m copying pycuda/sparse/coordinate.py -> build/lib.linux-x86_64-cpython-310/pycuda/sparse\n",
      "  \u001b[31m   \u001b[0m copying pycuda/sparse/packeted.py -> build/lib.linux-x86_64-cpython-310/pycuda/sparse\n",
      "  \u001b[31m   \u001b[0m copying pycuda/sparse/cg.py -> build/lib.linux-x86_64-cpython-310/pycuda/sparse\n",
      "  \u001b[31m   \u001b[0m copying pycuda/sparse/operator.py -> build/lib.linux-x86_64-cpython-310/pycuda/sparse\n",
      "  \u001b[31m   \u001b[0m copying pycuda/sparse/__init__.py -> build/lib.linux-x86_64-cpython-310/pycuda/sparse\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/pycuda/compyte\n",
      "  \u001b[31m   \u001b[0m copying pycuda/compyte/array.py -> build/lib.linux-x86_64-cpython-310/pycuda/compyte\n",
      "  \u001b[31m   \u001b[0m copying pycuda/compyte/dtypes.py -> build/lib.linux-x86_64-cpython-310/pycuda/compyte\n",
      "  \u001b[31m   \u001b[0m copying pycuda/compyte/__init__.py -> build/lib.linux-x86_64-cpython-310/pycuda/compyte\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing pycuda.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to pycuda.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to pycuda.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to pycuda.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'pycuda.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching 'doc/source/_static/*.css'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching 'doc/source/_templates/*.html'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '*.cpp' under directory 'bpl-subset/bpl_subset/boost'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '*.html' under directory 'bpl-subset/bpl_subset/boost'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '*.inl' under directory 'bpl-subset/bpl_subset/boost'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '*.txt' under directory 'bpl-subset/bpl_subset/boost'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '*.h' under directory 'bpl-subset/bpl_subset/libs'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '*.ipp' under directory 'bpl-subset/bpl_subset/libs'\n",
      "  \u001b[31m   \u001b[0m warning: no files found matching '*.pl' under directory 'bpl-subset/bpl_subset/libs'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'pycuda.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m /tmp/pip-build-env-dlt49o47/overlay/lib/python3.10/site-packages/setuptools/command/build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'pycuda.cuda' as data is deprecated, please list it in `packages`.\n",
      "  \u001b[31m   \u001b[0m     !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     ############################\n",
      "  \u001b[31m   \u001b[0m     # Package would be ignored #\n",
      "  \u001b[31m   \u001b[0m     ############################\n",
      "  \u001b[31m   \u001b[0m     Python recognizes 'pycuda.cuda' as an importable package,\n",
      "  \u001b[31m   \u001b[0m     but it is not listed in the `packages` configuration of setuptools.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     'pycuda.cuda' has been automatically added to the distribution only\n",
      "  \u001b[31m   \u001b[0m     because it may contain data files, but this behavior is likely to change\n",
      "  \u001b[31m   \u001b[0m     in future versions of setuptools (and therefore is considered deprecated).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     Please make sure that 'pycuda.cuda' is included as a package by using\n",
      "  \u001b[31m   \u001b[0m     the `packages` configuration field or the proper discovery methods\n",
      "  \u001b[31m   \u001b[0m     (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "  \u001b[31m   \u001b[0m     instead of `find_packages(...)`/`find:`).\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m     You can read more about \"package discovery\" and \"data files\" on setuptools\n",
      "  \u001b[31m   \u001b[0m     documentation page.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m   check.warn(importable)\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-310/pycuda/cuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/cuda/pycuda-complex-impl.hpp -> build/lib.linux-x86_64-cpython-310/pycuda/cuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/cuda/pycuda-complex.hpp -> build/lib.linux-x86_64-cpython-310/pycuda/cuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/cuda/pycuda-helpers.hpp -> build/lib.linux-x86_64-cpython-310/pycuda/cuda\n",
      "  \u001b[31m   \u001b[0m copying pycuda/sparse/pkt_build_cython.pyx -> build/lib.linux-x86_64-cpython-310/pycuda/sparse\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building '_driver' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs/python\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs/python/src\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs/python/src/converter\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs/python/src/object\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs/smart_ptr\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs/smart_ptr/src\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs/system\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs/system/src\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs/thread\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs/thread/src\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs/thread/src/pthread\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/src\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/src/cpp\n",
      "  \u001b[31m   \u001b[0m creating build/temp.linux-x86_64-cpython-310/src/wrapper\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /home/supremusdominus/anaconda3/envs/ML/compiler_compat -Wno-unused-result -Wsign-compare -fwrapv -Wall -O3 -DNDEBUG -fPIC -DBOOST_ALL_NO_LIB=1 -DBOOST_THREAD_BUILD_DLL=1 -DBOOST_MULTI_INDEX_DISABLE_SERIALIZATION=1 -DBOOST_PYTHON_SOURCE=1 -Dboost=pycudaboost -DBOOST_THREAD_DONT_USE_CHRONO=1 -DPYGPU_PACKAGE=pycuda -DPYGPU_PYCUDA=1 -DHAVE_CURAND=1 -Isrc/cpp -Ibpl-subset/bpl_subset \"-I/mnt/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.7/include\" -I/tmp/pip-build-env-dlt49o47/overlay/lib/python3.10/site-packages/numpy/core/include -I/home/supremusdominus/anaconda3/envs/ML/include/python3.10 -c bpl-subset/bpl_subset/libs/python/src/converter/arg_to_python_base.cpp -o build/temp.linux-x86_64-cpython-310/bpl-subset/bpl_subset/libs/python/src/converter/arg_to_python_base.o\n",
      "  \u001b[31m   \u001b[0m error: command 'gcc' failed: No such file or directory\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for pycuda\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build pycuda\n",
      "\u001b[31mERROR: Could not build wheels for pycuda, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rl-warp-drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bff9430",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'warp_drive'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mai_economist\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfoundation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mscenarios\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcovid19\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcovid19_env\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     CovidAndEconomyEnvironment,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mai_economist\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfoundation\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv_wrapper\u001b[39;00m \u001b[39mimport\u001b[39;00m FoundationEnvWrapper\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwarp_drive\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv_cpu_gpu_consistency_checker\u001b[39;00m \u001b[39mimport\u001b[39;00m EnvironmentCPUvsGPU\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwarp_drive\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtrainer\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwarp_drive\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m create_and_push_data_placeholders\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'warp_drive'"
     ]
    }
   ],
   "source": [
    "import ai_economist\n",
    "import numpy as np\n",
    "import os\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import dates as mdates\n",
    "from datetime import timedelta\n",
    "from timeit import Timer\n",
    "\n",
    "from ai_economist.foundation.scenarios.covid19.covid19_env import (\n",
    "    CovidAndEconomyEnvironment,\n",
    ")\n",
    "from ai_economist.foundation.env_wrapper import FoundationEnvWrapper\n",
    "\n",
    "from warp_drive.env_cpu_gpu_consistency_checker import EnvironmentCPUvsGPU\n",
    "from warp_drive.training.trainer import Trainer\n",
    "from warp_drive.training.utils.data_loader import create_and_push_data_placeholders\n",
    "from warp_drive.utils.env_registrar import EnvironmentRegistrar\n",
    "\n",
    "_PATH_TO_AI_ECONOMIST_PACKAGE_DIR = ai_economist.__path__[0]\n",
    "\n",
    "# Set font size for the matplotlib figures\n",
    "plt.rcParams.update({'font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logger level e.g., DEBUG, INFO, WARNING, ERROR\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34e846",
   "metadata": {},
   "source": [
    "# 1. Building a GPU-Compatible Environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cbb4a4",
   "metadata": {},
   "source": [
    "We start with a Python environment that has the [Gym](https://gym.openai.com/docs/)-style `__init__`, `reset` and `step` APIs. For example, consider the [COVID-19 economic simulation](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/scenarios/covid19/covid19_env.py). To build a GPU-compatible environment that can be trained with WarpDrive, you will need to first implement the simulation itself in CUDA C. While there are other alternatives for GPU-based simulations such as [Numba](https://numba.readthedocs.io/en/stable/cuda/index.html) and [JAX](https://jax.readthedocs.io/en/latest/), CUDA C provides the most flexibility for building complex multi-agent simulation logic, and also the fastest performance. However, implementing the simulation in\n",
    "CUDA C also requires the GPU memory and threads to be carefully managed. Some pointers on building and testing the simulation in CUDA C are provided in this WarpDrive [tutorial](https://github.com/salesforce/warp-drive/blob/master/tutorials/tutorial-4-create_custom_environments.md).\n",
    "\n",
    "Important: when writing the step function using CUDA C, the function names should follow the following convention so that they can be used with WarpDrive APIs.\n",
    "- The scenario class needs to have a 'name' attribute. The scenario step function requires to be named as \"Cuda{scenario_name}Step\".\n",
    "- Every component class needs to have a 'name' attribute. The step function for the component in the scenario requires to be named as \"Cuda{component_name}Step\".\n",
    "- The function used to compute the rewards requires to be named as \"CudaComputeReward\".\n",
    "\n",
    "The code for the COVID-19 economic simulation's step function is [here](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/scenarios/covid19/covid19_env_step.cu).\n",
    "\n",
    "To use an existing Python Environment with WarpDrive, one needs to add two augmentations (see below) to the Python code. First, a `get_data_dictionary()` method that pushes all the data arrays and environment parameters required to run the simulation to the GPU. Second, the step-function should invoke the `cuda_step` kernel with the data arrays that the CUDA C step function should have access to passed as arguments.\n",
    "\n",
    "```python\n",
    "class Env:\n",
    "    def __init__(self, **env_config):\n",
    "        ...\n",
    "\n",
    "    def reset(self):\n",
    "        ...\n",
    "        return obs\n",
    "\n",
    "    def get_data_dictionary(self):\n",
    "        # Specify the data that needs to be \n",
    "        # pushed to the GPU.\n",
    "        data_feed = DataFeed()\n",
    "        data_feed.add_data(\n",
    "            name=\"variable_name\",\n",
    "            data=self.variable,\n",
    "            save_copy_and_apply_at_reset\n",
    "            =True,\n",
    "        )\n",
    "        ...\n",
    "        return data_feed\n",
    "\n",
    "    def step(self, actions):\n",
    "        if self.use_cuda:\n",
    "            self.cuda_step(\n",
    "                # Pass the relevant data \n",
    "                # feed keys as arguments \n",
    "                # to cuda_step. \n",
    "                # Note: cuda_data_manager \n",
    "                # is created by the \n",
    "                # EnvWrapper.\n",
    "                self.cuda_data_manager.\n",
    "                device_data(...),\n",
    "                ...\n",
    "            )\n",
    "        else:\n",
    "            ...\n",
    "            return obs, rew, done, info\n",
    "```\n",
    "The complete Python code is [here](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/scenarios/covid19/covid19_env.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4645b43",
   "metadata": {},
   "source": [
    "# 2. CPU-GPU Environment Consistency Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9b8ae",
   "metadata": {},
   "source": [
    "Before we train the simulation on the GPU, we will need to ensure consistency between the Python and CUDA C versions of the simulation. For this purpose, Foundation provides an [EnvironmentCPUvsGPU class](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/env_cpu_gpu_consistency_checker.py). This module essentially instantiates environment objects corresponding to the two versions of the simulation. It then steps through the two environment objects for a specified number of environment replicas `num_envs` and a specified number of episodes `num_episodes`, and verifies that the observations, actions, rewards and the “done” flags are the same after each step. We have created a testing [script](https://github.com/salesforce/ai-economist/blob/master/tests/run_covid19_cpu_gpu_consistency_checks.py) that performs the consistency checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a990b032",
   "metadata": {},
   "source": [
    "First, we will create an environment configuration to test with. For more details on what the configuration parameters mean, please refer to the simulation [code](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/scenarios/covid19/covid19_env.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1943025",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    'collate_agent_step_and_reset_data': True,\n",
    "     'components': [\n",
    "         {'ControlUSStateOpenCloseStatus': {'action_cooldown_period': 28}},\n",
    "          {'FederalGovernmentSubsidy': {'num_subsidy_levels': 20,\n",
    "            'subsidy_interval': 90,\n",
    "            'max_annual_subsidy_per_person': 20000}},\n",
    "          {'VaccinationCampaign': {'daily_vaccines_per_million_people': 3000,\n",
    "            'delivery_interval': 1,\n",
    "            'vaccine_delivery_start_date': '2021-01-12'}},\n",
    "        {\"FederalQuantitativeEasing\": {\n",
    "            # The number of QE levels.\n",
    "            \"num_QE_levels\": 20,\n",
    "            # The number of days over which the total subsidy amount is evenly rolled out.\n",
    "            \"QE_interval\": 90,\n",
    "            # The maximum annual subsidy that may be allocated per person.\n",
    "            \"max_annual_QE_per_person\": 20\n",
    "        }},\n",
    "     ],\n",
    "     'economic_reward_crra_eta': 2,\n",
    "     'episode_length': 540,\n",
    "     'flatten_masks': True,\n",
    "     'flatten_observations': False,\n",
    "     'health_priority_scaling_agents': 0.3,\n",
    "     'health_priority_scaling_planner': 0.45,\n",
    "     'infection_too_sick_to_work_rate': 0.1,\n",
    "     'multi_action_mode_agents': False,\n",
    "     'multi_action_mode_planner': False,\n",
    "     'n_agents': 51,\n",
    "     'path_to_data_and_fitted_params': '',\n",
    "     'pop_between_age_18_65': 0.6,\n",
    "     'risk_free_interest_rate': 0.03,\n",
    "     'world_size': [1, 1],\n",
    "     'start_date': '2020-03-22',\n",
    "     'use_real_world_data': False,\n",
    "     'use_real_world_policies': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364047c4",
   "metadata": {},
   "source": [
    "Next, we will need to register the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5f0e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_registrar = EnvironmentRegistrar()\n",
    "env_registrar.add_cuda_env_src_path(\n",
    "    CovidAndEconomyEnvironment.name,\n",
    "    os.path.join(\n",
    "        _PATH_TO_AI_ECONOMIST_PACKAGE_DIR, \n",
    "        \"foundation/scenarios/covid19/covid19_build.cu\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213dbdf0",
   "metadata": {},
   "source": [
    "We will also need to set some variables: `policy_tag_to_agent_id_map`, `separate_placeholder_per_policy` (defaults to False) and `obs_dim_corresponding_to_num_agents` (defaults to \"first\"). The variable descriptions are below in comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7556b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The policy_tag_to_agent_id_map dictionary maps\n",
    "# policy model names to agent ids.\n",
    "policy_tag_to_agent_id_map = {\n",
    "    \"a\": [str(agent_id) for agent_id in range(env_config[\"n_agents\"])],\n",
    "    \"p\": [\"p\"],\n",
    "    \"f\": [\"f\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag indicating whether separate obs, actions and rewards placeholders have to be created for each policy.\n",
    "# Set \"create_separate_placeholders_for_each_policy\" to True here \n",
    "# since the agents and the planner have different observation and action spaces.\n",
    "separate_placeholder_per_policy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd2468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag indicating the observation dimension corresponding to 'num_agents'.\n",
    "# Note: WarpDrive assumes that all the observation are shaped\n",
    "# (num_agents, *feature_dim), i.e., the observation dimension\n",
    "# corresponding to 'num_agents' is the first one. Instead, if the\n",
    "# observation dimension corresponding to num_agents is the last one,\n",
    "# we will need to permute the axes to align with WarpDrive's assumption\n",
    "obs_dim_corresponding_to_num_agents = \"last\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f128d8",
   "metadata": {},
   "source": [
    "The consistency tests may be performed using the `test_env_reset_and_step()` API as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1898dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnvironmentCPUvsGPU(\n",
    "    dual_mode_env_class=CovidAndEconomyEnvironment,\n",
    "    env_configs={\"test\": env_config},\n",
    "    num_envs=3,\n",
    "    num_episodes=2,\n",
    "    env_wrapper=FoundationEnvWrapper,\n",
    "    env_registrar=env_registrar,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    "    create_separate_placeholders_for_each_policy=True,\n",
    "    obs_dim_corresponding_to_num_agents=\"last\"    \n",
    ").test_env_reset_and_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867963d",
   "metadata": {},
   "source": [
    "If the two implementations are consistent, you should see  `The CPU and the GPU environment outputs are consistent within 1 percent` at the end of the previous cell run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83bf539",
   "metadata": {},
   "source": [
    "# 3. Adding an *Environment Wrapper*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7f438",
   "metadata": {},
   "source": [
    "Once the Python and CUDA C implementation are consistent with one another, we use an environment wrapper to wrap the environment object, and run the simulation on the GPU. Accordingly, we need to set the `use_cuda` argument to True. under this setting, only the first environment reset happens on the CPU. Following that, the data arrays created at reset and the simulation parameters are copied over (a one-time operation) to the GPU memory. All the subsequent steps (and resets) happen only on the GPU. In other words, there's no back-and-forth data copying between the CPU and the GPU, and all the data arrays on the GPU are modified in-place. The environment wrapper also uses the `num_envs` argument (defaults to $1$) to instantiate multiple replicas of the environment on the GPU.\n",
    "\n",
    "Note: for running the simulation on a CPU, simply set use_cuda=False, and it is no different than actually running the Python simulation on a CPU - the reset and step calls also happen on the CPU.\n",
    "\n",
    "The environment wrapper essentially performs the following tasks that are required to run the simulation on the GPU:\n",
    "- Registers the CUDA step kernel, so that the step function can be invoked from the CPU (host).\n",
    "- Pushes all the data listed in the data dictionary to the GPU when the environment is reset for the very frst time.\n",
    "- Automatically resets every environment when it reaches its done state.\n",
    "- Adds the observation and action spaces to the environment, which are required when training the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede05b74",
   "metadata": {},
   "source": [
    "The CPU and GPU versions of the environment object may be created via setting the appropriate value of the `use_cuda` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db1010",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_env = FoundationEnvWrapper(\n",
    "    CovidAndEconomyEnvironment(**env_config),\n",
    "    use_cuda=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa259b4e",
   "metadata": {},
   "source": [
    "Instantiating the GPU environment also initializes the data function managers and loads the CUDA kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e433e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_env = FoundationEnvWrapper(\n",
    "    CovidAndEconomyEnvironment(**env_config),\n",
    "    use_cuda=True,\n",
    "    env_registrar=env_registrar,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a204d966",
   "metadata": {},
   "source": [
    "# 4. Creating a Trainer Object and Perform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbea778",
   "metadata": {},
   "source": [
    "Next, we will prepare the environment for training on a GPU.We will need to define a run configuration (which comprises the environment, training, policy and saving configurations), and create a *trainer* object.\n",
    "\n",
    "We will load the run configuration from a saved yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ee947",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(\n",
    "    _PATH_TO_AI_ECONOMIST_PACKAGE_DIR,\n",
    "    \"training/run_configs/\",\n",
    "    f\"covid_and_economy_environment.yaml\",\n",
    ")\n",
    "with open(config_path, \"r\", encoding=\"utf8\") as f:\n",
    "    run_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed835bbd",
   "metadata": {},
   "source": [
    "### Instantiating the Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c758763",
   "metadata": {},
   "source": [
    "Next, we will create and instantiate the trainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f241ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    env_wrapper=cpu_env,\n",
    "    config=run_config,\n",
    "    policy_tag_to_agent_id_map=policy_tag_to_agent_id_map,\n",
    "    create_separate_placeholders_for_each_policy=separate_placeholder_per_policy,\n",
    "    obs_dim_corresponding_to_num_agents=obs_dim_corresponding_to_num_agents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb37f72",
   "metadata": {},
   "source": [
    "### CPU vs GPU Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a582b81",
   "metadata": {},
   "source": [
    "Before performing training, let us see how the simulation speed on the GPU compares with that of the CPU. We will generate a set of random actions, and step through both versions of the simulation a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fdb713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_actions(env):\n",
    "    actions = {\n",
    "            str(agent_id): np.random.randint(\n",
    "                low=0,\n",
    "                high=env.env.action_space[str(agent_id)].n,\n",
    "                dtype=np.int32,\n",
    "            )\n",
    "            for agent_id in range(env.n_agents-1)\n",
    "    }\n",
    "    actions[\"p\"] = np.random.randint(\n",
    "        low=0,\n",
    "        high=env.env.action_space[\"p\"].n,\n",
    "        dtype=np.int32,\n",
    "    )\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cd89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_reset_and_step(env):\n",
    "    env.reset()\n",
    "    actions = generate_random_actions(env)\n",
    "    for t in range(env.episode_length):\n",
    "        env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dd1cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "Timer(lambda: env_reset_and_step(gpu_env)).timeit(number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Timer(lambda: env_reset_and_step(cpu_env)).timeit(number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626a15a",
   "metadata": {},
   "source": [
    "Notice that with just $1$ replica of the environment, the environment step on the GPU is over 5x faster (on an A100 machine). When running training, it is typical to use several environment replicas, and that provides an even higher performance boost for the GPU, since WarpDrive runs all the environment replicas in parallel on separate GPU blocks, and the CPU cannot achieve the same amount of parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6846a",
   "metadata": {},
   "source": [
    "### Perform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0aac10",
   "metadata": {},
   "source": [
    "We perform training by invoking `trainer.train()`. The speed performance stats and metrics for the trained policies are printed on screen.\n",
    "Note: In this notebook, we only run training for $200$ iterations. You may run it for longer by setting the `num_episodes` configuration parameter [here](https://github.com/salesforce/ai-economist/blob/master/ai_economist/training/run_configs/covid_and_economy_environment.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f71acf9",
   "metadata": {},
   "source": [
    "# 5. Visualize the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0973cb",
   "metadata": {},
   "source": [
    "Post training, it is useful to visualize some of the environment's actions and observations to gain more insight into the kinds of policies the RL agents learn, and the resulting environment dynamics. For the COVID-19 and economic simulation, the actions - \"stringency level\" and \"subsidy level\" control the observables such such as \"susceptible\", \"infected\", \"recovered\", \"deaths\", \"unemployed\", \"vaccinated\" and \"productivity\" for each of the US states.\n",
    "\n",
    "Incidentally, these actions and observables also correspond to the names of arrays that were pushed to the GPU after the very first environment reset. At any time, the arrays can be fetched back to the CPU via the WarpDrive trainer's API `fetch_episode_states`, and visualized for the duration of an entire episode. Below, we also provide a helper function to perform the visualizations. Note that in this notebook, we only performed a few iterations of training, so the policies will not be quite trained at this point, so the plots seen in the visualization seen are going to be arbitrary. You may run training with a different set of configurations or for longer by setting the `num_episodes` configuration parameter [here](https://github.com/salesforce/ai-economist/blob/master/ai_economist/training/run_configs/covid_and_economy_environment.yaml), and you can visualize the policies after, using the same code provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f755a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: visualizations\n",
    "\n",
    "def visualize_states(\n",
    "    entity=\"USA\",\n",
    "    episode_states=None,\n",
    "    trainer=None,\n",
    "    ax=None\n",
    "):\n",
    "    assert trainer is not None\n",
    "    assert episode_states is not None\n",
    "       \n",
    "    # US state names to index mapping\n",
    "    us_state_name_to_idx = {v: k for k, v in trainer.cuda_envs.env.us_state_idx_to_state_name.items()}\n",
    "    us_state_name_to_idx[\"USA\"] = \"p\"\n",
    "    agent_id = us_state_name_to_idx[entity]\n",
    "    \n",
    "    assert entity is not None\n",
    "    assert entity in us_state_name_to_idx.keys(), f\"entity should be in {list(us_state_name_to_idx.keys())}\"\n",
    "    \n",
    "    for key in episode_states:\n",
    "        # Use the collated data at the last valid time index\n",
    "        last_valid_time_index = np.isnan(np.sum(episode_states[key], axis = (1, 2))).argmax() - 1\n",
    "        episode_states[key] = episode_states[key][last_valid_time_index]\n",
    "    \n",
    "    if agent_id == \"p\":\n",
    "        for key in episode_states:\n",
    "                \n",
    "            if key in [\"subsidy_level\", \"stringency_level\"]:\n",
    "                episode_states[key] = np.mean(episode_states[key], axis=-1) # average across all the US states\n",
    "            else:\n",
    "                episode_states[key] = np.sum(episode_states[key], axis=-1) # sum across all the US states\n",
    "            episode_states[key] = episode_states[key].reshape(-1, 1) # putting back the agent_id dimension\n",
    "        agent_id = 0\n",
    "    else:\n",
    "        agent_id = int(agent_id)\n",
    "        \n",
    "    if ax is None:\n",
    "        if len(episode_states) < 3:\n",
    "            cols = len(episode_states)\n",
    "        else:\n",
    "            cols = 3\n",
    "        scale = 8\n",
    "        rows = int(np.ceil(len(episode_states) / cols))\n",
    "    \n",
    "        h, w = scale*max(rows, cols), scale*max(rows, rows)\n",
    "        fig, ax = plt.subplots(rows, cols, figsize=(h, w), sharex=True, squeeze=False)\n",
    "    else:\n",
    "        rows, cols = ax.shape\n",
    "        \n",
    "    start_date = trainer.cuda_envs.env.start_date\n",
    "    \n",
    "    for idx, key in enumerate(episode_states):\n",
    "        row = idx // cols\n",
    "        col = idx % cols\n",
    "\n",
    "        dates = [start_date + timedelta(day) for day in range(episode_states[key].shape[0])]\n",
    "        ax[row][col].plot(dates, episode_states[key][:, agent_id], linewidth=3)\n",
    "        ax[row][col].set_ylabel(key)\n",
    "        ax[row][col].grid(b=True)\n",
    "        ax[row][col].xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        ax[row][col].xaxis.set_major_formatter(mdates.DateFormatter(\"%b'%y\"))\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa83589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the key state indicators for an episode.\n",
    "episode_states = trainer.fetch_episode_states(\n",
    "    [\n",
    "        \"stringency_level\",\n",
    "        \"subsidy_level\",        \n",
    "        \"susceptible\",\n",
    "        \"infected\",\n",
    "        \"recovered\",\n",
    "        \"deaths\",\n",
    "        \"unemployed\",\n",
    "        \"vaccinated\",\n",
    "        \"productivity\",\n",
    "        \"postsubsidy_productivity\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Visualize the fetched states for the USA.\n",
    "# Feel free to modify the 'entity' argument to visualize the curves for the US states (e.g., California, Utah) too.\n",
    "visualize_states(\n",
    "    entity=\"USA\",\n",
    "    episode_states=episode_states,\n",
    "    trainer=trainer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1649b9c5",
   "metadata": {},
   "source": [
    "And that's it for this tutorial. Happy training with Foundation and WarpDrive!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ML': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9dbfbadc71e42abc9cd558c210cdff0e47003e6a6f6a3b108a42ec1174a9608f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
