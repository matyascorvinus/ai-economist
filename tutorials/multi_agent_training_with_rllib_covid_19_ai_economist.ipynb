{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTETdipRKslE"
   },
   "source": [
    "Copyright (c) 2021, salesforce.com, inc.  \n",
    "All rights reserved.  \n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db18ta9YKslH"
   },
   "source": [
    "### Colab\n",
    "\n",
    "Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/ai-economist/blob/master/tutorials/multi_agent_training_with_rllib.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7u8q3nesKslH"
   },
   "source": [
    "### Prerequisites\n",
    "It is helpful to be familiar with **Foundation**, a multi-agent economic simulator built for the AI Economist ([paper here](https://arxiv.org/abs/2004.13332)). If you haven't worked with Foundation before, we highly recommend taking a look at our other tutorials:\n",
    "\n",
    "- [Foundation: the Basics](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb)\n",
    "- [Extending Foundation](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_advanced.ipynb)\n",
    "- [Optimal Taxation Theory and Simulation](https://github.com/salesforce/ai-economist/blob/master/tutorials/optimal_taxation_theory_and_simulation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGkKf0toKslI"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxskLNk1KslI"
   },
   "source": [
    "Welcome! This tutorial is the first of a series on doing distributed multi-agent reinforcement learning (MARL). Here, we specifically demonstrate how to integrate our multi-agent economic simulation, [Foundation](https://github.com/salesforce/ai-economist/tree/master/ai_economist/foundation), with [RLlib](https://github.com/ray-project/ray/tree/master/rllib), an open-source library for reinforcement learning. We chose to use RLlib, as it provides an easy-to-use and flexible library for MARL. A detailed documentation on RLlib is available [here](https://docs.ray.io/en/master/rllib.html).\n",
    "\n",
    "We put together these tutorial notebook with the following key goals in mind:\n",
    "- Provide an exposition to MARL. While there are many libraries and references out there for single-agent RL training, MARL training is not discussed as much, and there aren't many multi-agent rl libraries.\n",
    "- Provide reference starting code to perform MARL training so the AI Economist community can focus more on building meaningful extensions to Foundation and better-performant algorithms.\n",
    "\n",
    "We will cover the following concepts in this tutorial:\n",
    "1. Adding an *environment wrapper* to make the economic simulation compatible with RLlib.\n",
    "2. Creating a *trainer* object that holds the (multi-agent) policies for environment interaction.\n",
    "3. Training all the agents in the economic simulation.\n",
    "4. Generate a rollout using the trainer object and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4JZv1MbKslJ"
   },
   "source": [
    "### Dependencies:\n",
    "You can install the ai-economist package using \n",
    "- the pip package manager OR\n",
    "- by cloning the ai-economist package and installing the requirements (we shall use this when running on Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T21:06:29.198112Z",
     "start_time": "2023-05-02T21:06:28.792156Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhRHQptlKslK",
    "outputId": "a32e75fa-e086-427f-9ac9-add555aa12cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/supremusdominus/Download/ai-economist\n",
      "Requirement already satisfied: pip in /home/supremusdominus/.local/lib/python3.7/site-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Using cached pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "Successfully installed pip-23.3.1\n",
      "Obtaining file:///home/supremusdominus/Download/ai-economist\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: appdirs==1.4.4 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.4.4)\n",
      "Requirement already satisfied: appnope==0.1.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.1.2)\n",
      "Requirement already satisfied: argon2-cffi==20.1.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (20.1.0)\n",
      "Requirement already satisfied: astroid==2.5.6 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2.5.6)\n",
      "Requirement already satisfied: async-generator==1.10 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.10)\n",
      "Requirement already satisfied: attrs==21.2.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (21.2.0)\n",
      "Requirement already satisfied: backcall==0.2.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.9.3 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (4.9.3)\n",
      "Requirement already satisfied: black==21.5b1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (21.5b1)\n",
      "Requirement already satisfied: bleach==3.3.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (3.3.0)\n",
      "Requirement already satisfied: bs4==0.0.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.0.1)\n",
      "Requirement already satisfied: certifi==2020.12.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2020.12.5)\n",
      "Requirement already satisfied: cffi==1.14.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.14.5)\n",
      "Requirement already satisfied: chardet==4.0.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (4.0.0)\n",
      "Requirement already satisfied: click==8.0.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (8.0.1)\n",
      "Requirement already satisfied: cycler==0.10.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.10.0)\n",
      "Requirement already satisfied: decorator==5.0.9 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (5.0.9)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.7.1)\n",
      "Collecting entrypoints==0.3 (from ai-economist==1.7.1)\n",
      "  Using cached entrypoints-0.3-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: et-xmlfile==1.1.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.1.0)\n",
      "Requirement already satisfied: flake8==3.9.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (3.9.2)\n",
      "Requirement already satisfied: GPUtil==1.4.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.4.0)\n",
      "Requirement already satisfied: idna==2.10 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2.10)\n",
      "Requirement already satisfied: iniconfig==1.1.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.1.1)\n",
      "Requirement already satisfied: ipykernel==5.5.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (5.5.5)\n",
      "Collecting ipython==7.31.1 (from ai-economist==1.7.1)\n",
      "  Using cached ipython-7.31.1-py3-none-any.whl (792 kB)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.2.0)\n",
      "Requirement already satisfied: ipywidgets==7.6.3 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (7.6.3)\n",
      "Requirement already satisfied: isort==5.8.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (5.8.0)\n",
      "Requirement already satisfied: jedi==0.18.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.18.0)\n",
      "Requirement already satisfied: Jinja2==3.0.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (3.0.1)\n",
      "Collecting jsonschema==3.2.0 (from ai-economist==1.7.1)\n",
      "  Using cached jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: jupyter==1.0.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.0.0)\n",
      "Requirement already satisfied: jupyter-client==6.1.12 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (6.1.12)\n",
      "Requirement already satisfied: jupyter-console==6.4.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (6.4.0)\n",
      "Requirement already satisfied: jupyter-core==4.7.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (4.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments==0.1.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.1.2)\n",
      "Requirement already satisfied: jupyterlab-widgets==1.0.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.0.0)\n",
      "Requirement already satisfied: kiwisolver==1.3.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.3.1)\n",
      "Requirement already satisfied: lazy-object-proxy==1.6.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.6.0)\n",
      "Requirement already satisfied: lz4==3.1.3 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (3.1.3)\n",
      "Collecting MarkupSafe==2.0.1 (from ai-economist==1.7.1)\n",
      "  Downloading MarkupSafe-2.0.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (31 kB)\n",
      "Requirement already satisfied: matplotlib==3.2.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (3.2.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.1.2)\n",
      "Requirement already satisfied: mccabe==0.6.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.6.1)\n",
      "Requirement already satisfied: mistune==0.8.4 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.8.4)\n",
      "Requirement already satisfied: mypy-extensions==0.4.3 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.4.3)\n",
      "Requirement already satisfied: nbclient==0.5.3 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.5.3)\n",
      "Requirement already satisfied: nbconvert==6.0.7 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (6.0.7)\n",
      "Requirement already satisfied: nbformat==5.1.3 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (5.1.3)\n",
      "Requirement already satisfied: nest-asyncio==1.5.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.5.1)\n",
      "Requirement already satisfied: notebook==6.4.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (6.4.1)\n",
      "Requirement already satisfied: numpy==1.21.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.21.0)\n",
      "Requirement already satisfied: openpyxl==3.0.7 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (3.0.7)\n",
      "Requirement already satisfied: packaging==20.9 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (20.9)\n",
      "Requirement already satisfied: pandas==1.2.4 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.2.4)\n",
      "Requirement already satisfied: pandocfilters==1.4.3 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.4.3)\n",
      "Requirement already satisfied: parso==0.8.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.8.2)\n",
      "Requirement already satisfied: pathspec==0.8.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.8.1)\n",
      "Requirement already satisfied: pexpect==4.8.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (4.8.0)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.7.5)\n",
      "Requirement already satisfied: Pillow==9.0.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (9.0.1)\n",
      "Requirement already satisfied: pluggy==0.13.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.13.1)\n",
      "Requirement already satisfied: prometheus-client==0.10.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.10.1)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.18 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (3.0.18)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.7.0)\n",
      "Requirement already satisfied: py==1.10.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.10.0)\n",
      "Requirement already satisfied: pycodestyle==2.7.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2.7.0)\n",
      "Requirement already satisfied: pycparser==2.20 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2.20)\n",
      "Requirement already satisfied: pycryptodome==3.10.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (3.10.1)\n",
      "Requirement already satisfied: pyflakes==2.3.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2.3.1)\n",
      "Requirement already satisfied: Pygments==2.9.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2.9.0)\n",
      "Requirement already satisfied: pylint==2.8.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2.8.2)\n",
      "Requirement already satisfied: pyparsing==2.4.7 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2.4.7)\n",
      "Requirement already satisfied: pyrsistent==0.17.3 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.17.3)\n",
      "Requirement already satisfied: pytest==6.2.4 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (6.2.4)\n",
      "Requirement already satisfied: python-dateutil==2.8.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2.8.1)\n",
      "Requirement already satisfied: pytz==2021.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2021.1)\n",
      "Requirement already satisfied: pyyaml==5.4.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (5.4.1)\n",
      "Requirement already satisfied: pyzmq==22.0.3 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (22.0.3)\n",
      "Requirement already satisfied: qtconsole==5.1.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (5.1.0)\n",
      "Requirement already satisfied: QtPy==1.9.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.9.0)\n",
      "Requirement already satisfied: regex==2021.4.4 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2021.4.4)\n",
      "Requirement already satisfied: requests==2.25.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2.25.1)\n",
      "Requirement already satisfied: scipy==1.6.3 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.6.3)\n",
      "Requirement already satisfied: Send2Trash==1.5.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.5.0)\n",
      "Requirement already satisfied: six==1.16.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.16.0)\n",
      "Requirement already satisfied: soupsieve==2.2.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (2.2.1)\n",
      "Requirement already satisfied: terminado==0.10.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.10.0)\n",
      "Requirement already satisfied: testpath==0.5.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.5.0)\n",
      "Requirement already satisfied: toml==0.10.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.10.2)\n",
      "Requirement already satisfied: tornado==6.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (6.1)\n",
      "Requirement already satisfied: tqdm==4.60.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (4.60.0)\n",
      "Requirement already satisfied: traitlets==5.0.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (5.0.5)\n",
      "Collecting typing-extensions==3.10.0.0 (from ai-economist==1.7.1)\n",
      "  Using cached typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: urllib3==1.26.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.26.5)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.2.5)\n",
      "Requirement already satisfied: webencodings==0.5.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (0.5.1)\n",
      "Requirement already satisfied: widgetsnbextension==3.5.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (3.5.1)\n",
      "Requirement already satisfied: wrapt==1.12.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ai-economist==1.7.1) (1.12.1)\n",
      "Requirement already satisfied: typed-ast<1.5,>=1.4.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from astroid==2.5.6->ai-economist==1.7.1) (1.4.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/supremusdominus/.local/lib/python3.7/site-packages (from click==8.0.1->ai-economist==1.7.1) (6.0.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (from ipython==7.31.1->ai-economist==1.7.1) (68.2.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from importlib-metadata->click==8.0.1->ai-economist==1.7.1) (3.15.0)\n",
      "Installing collected packages: typing-extensions, MarkupSafe, entrypoints, jsonschema, ipython, ai-economist\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.2\n",
      "    Uninstalling MarkupSafe-2.1.2:\n",
      "      Successfully uninstalled MarkupSafe-2.1.2\n",
      "  Attempting uninstall: entrypoints\n",
      "    Found existing installation: entrypoints 0.4\n",
      "    Uninstalling entrypoints-0.4:\n",
      "      Successfully uninstalled entrypoints-0.4\n",
      "  Attempting uninstall: ipython\n",
      "    Found existing installation: ipython 7.33.0\n",
      "    Uninstalling ipython-7.33.0:\n",
      "      Successfully uninstalled ipython-7.33.0\n",
      "  Running setup.py develop for ai-economist\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ray 0.8.4 requires colorama, which is not installed.\n",
      "werkzeug 2.2.3 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.0.1 ai-economist entrypoints-0.3 ipython-7.31.1 jsonschema-3.2.0 typing-extensions-3.10.0.0\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /home/supremusdominus/.local/lib/python3.7/site-packages (from scikit-learn) (1.21.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from scikit-learn) (1.6.3)\n",
      "Collecting joblib>=0.11 (from scikit-learn)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.0.2 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "import os, signal, sys, time\n",
    "# IN_COLAB = 'google.colab' in sys.modules\n",
    "#\n",
    "# if IN_COLAB:\n",
    "#     !git clone https://github.com/matyascorvinus/ai-economist\n",
    "#\n",
    "#     %cd ai-economist\n",
    "#     !pip install -e .\n",
    "#\n",
    "#     # Restart the Python runtime to automatically use the installed packages\n",
    "#     print(\"\\n\\nRestarting the Python runtime! Please (re-)run the cells below.\")\n",
    "#     time.sleep(1)\n",
    "#     #os.kill(os.getpid(), signal.SIGKILL)\n",
    "# else:\n",
    "#     ! pip install ai-economist\n",
    "# pwd : '/home/supremusdominus/Download/ai-economist/tutorials'\n",
    "# %cd /home/supremusdominus/Download/ai-economist\n",
    "\n",
    "# ! pip install -e .\n",
    "\n",
    "INITIALIZE = True\n",
    "if INITIALIZE:\n",
    "    %cd /home/supremusdominus/Download/ai-economist\n",
    "    !pip install --upgrade pip\n",
    "    !pip install -e . \n",
    "    !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: colorama in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install colorama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xu9CtjfzKslL"
   },
   "source": [
    "Install OpenAI Gym to help define the environment's observation and action spaces for use with RLlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L6nUQInKslM"
   },
   "source": [
    "Install the `RLlib` reinforcement learning library:\n",
    "- First, install TensorFlow\n",
    "- Then, install ray[rllib]\n",
    "\n",
    "Note: RLlib natively supports TensorFlow (including TensorFlow Eager) as well as PyTorch, but most of its internals are framework agnostic. Here's a relevant [blogpost](https://medium.com/distributed-computing-with-ray/lessons-from-implementing-12-deep-rl-algorithms-in-tf-and-pytorch-1b412009297d) that compares running RLlib algorithms with TF and PyTorch. Overall, TF seems to run a bit faster than PyTorch, in our experience, and we will use that in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T21:09:10.275263Z",
     "start_time": "2023-05-02T21:07:21.159635Z"
    },
    "id": "JjbKL75JKslN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.21 in /home/supremusdominus/.local/lib/python3.7/site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from gym==0.21) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from gym==0.21) (1.21.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from gym==0.21) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym==0.21) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (from importlib-metadata>=4.8.1->gym==0.21) (3.10.0.0)\n",
      "Requirement already satisfied: tensorflow==1.14 in /home/supremusdominus/.local/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (1.4.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (0.8.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (0.5.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (0.2.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (1.1.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (1.21.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (3.20.3)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (1.14.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (2.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (1.12.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (1.51.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorflow==1.14) (0.38.4)\n",
      "Requirement already satisfied: h5py in /home/supremusdominus/.local/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==1.14) (3.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (68.2.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (2.2.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/supremusdominus/.local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14) (3.10.0.0)\n",
      "Requirement already satisfied: ray==0.8.4 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray[rllib]==0.8.4) (0.8.4)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray==0.8.4->ray[rllib]==0.8.4) (1.21.0)\n",
      "Requirement already satisfied: filelock in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray==0.8.4->ray[rllib]==0.8.4) (3.9.0)\n",
      "Requirement already satisfied: jsonschema in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (from ray==0.8.4->ray[rllib]==0.8.4) (3.2.0)\n",
      "Requirement already satisfied: click in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray==0.8.4->ray[rllib]==0.8.4) (8.0.1)\n",
      "Requirement already satisfied: colorama in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (from ray==0.8.4->ray[rllib]==0.8.4) (0.4.6)\n",
      "Requirement already satisfied: pyyaml in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray==0.8.4->ray[rllib]==0.8.4) (5.4.1)\n",
      "Requirement already satisfied: redis>=3.3.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray==0.8.4->ray[rllib]==0.8.4) (4.5.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray==0.8.4->ray[rllib]==0.8.4) (3.20.3)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray==0.8.4->ray[rllib]==0.8.4) (0.3.14)\n",
      "Requirement already satisfied: aiohttp in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray==0.8.4->ray[rllib]==0.8.4) (3.8.4)\n",
      "Requirement already satisfied: google in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray==0.8.4->ray[rllib]==0.8.4) (3.0.0)\n",
      "Requirement already satisfied: grpcio in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray==0.8.4->ray[rllib]==0.8.4) (1.51.3)\n",
      "Requirement already satisfied: tabulate in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray[rllib]==0.8.4) (0.9.0)\n",
      "Requirement already satisfied: tensorboardX in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray[rllib]==0.8.4) (2.6)\n",
      "Requirement already satisfied: pandas in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray[rllib]==0.8.4) (1.2.4)\n",
      "Requirement already satisfied: atari-py in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray[rllib]==0.8.4) (0.2.9)\n",
      "Requirement already satisfied: dm-tree in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray[rllib]==0.8.4) (0.1.8)\n",
      "Requirement already satisfied: gym[atari] in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray[rllib]==0.8.4) (0.21.0)\n",
      "Requirement already satisfied: lz4 in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray[rllib]==0.8.4) (3.1.3)\n",
      "Requirement already satisfied: opencv-python-headless in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray[rllib]==0.8.4) (4.1.2.30)\n",
      "Requirement already satisfied: scipy in /home/supremusdominus/.local/lib/python3.7/site-packages (from ray[rllib]==0.8.4) (1.6.3)\n",
      "Requirement already satisfied: async-timeout>=4.0.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from redis>=3.3.2->ray==0.8.4->ray[rllib]==0.8.4) (4.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=1.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from redis>=3.3.2->ray==0.8.4->ray[rllib]==0.8.4) (6.0.0)\n",
      "Requirement already satisfied: typing-extensions in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (from redis>=3.3.2->ray==0.8.4->ray[rllib]==0.8.4) (3.10.0.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from aiohttp->ray==0.8.4->ray[rllib]==0.8.4) (21.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from aiohttp->ray==0.8.4->ray[rllib]==0.8.4) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from aiohttp->ray==0.8.4->ray[rllib]==0.8.4) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from aiohttp->ray==0.8.4->ray[rllib]==0.8.4) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from aiohttp->ray==0.8.4->ray[rllib]==0.8.4) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from aiohttp->ray==0.8.4->ray[rllib]==0.8.4) (1.3.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from aiohttp->ray==0.8.4->ray[rllib]==0.8.4) (0.13.0)\n",
      "Requirement already satisfied: six in /home/supremusdominus/.local/lib/python3.7/site-packages (from atari-py->ray[rllib]==0.8.4) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/supremusdominus/.local/lib/python3.7/site-packages (from google->ray==0.8.4->ray[rllib]==0.8.4) (4.9.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.4) (2.2.1)\n",
      "Requirement already satisfied: ale-py~=0.7.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.4) (0.7.5)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from jsonschema->ray==0.8.4->ray[rllib]==0.8.4) (0.17.3)\n",
      "Requirement already satisfied: setuptools in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (from jsonschema->ray==0.8.4->ray[rllib]==0.8.4) (68.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/supremusdominus/.local/lib/python3.7/site-packages (from pandas->ray[rllib]==0.8.4) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/supremusdominus/.local/lib/python3.7/site-packages (from pandas->ray[rllib]==0.8.4) (2021.1)\n",
      "Requirement already satisfied: packaging in /home/supremusdominus/.local/lib/python3.7/site-packages (from tensorboardX->ray[rllib]==0.8.4) (20.9)\n",
      "Requirement already satisfied: importlib-resources in /home/supremusdominus/.local/lib/python3.7/site-packages (from ale-py~=0.7.1->gym[atari]; extra == \"rllib\"->ray[rllib]==0.8.4) (5.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from importlib-metadata>=1.0->redis>=3.3.2->ray==0.8.4->ray[rllib]==0.8.4) (3.15.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp->ray==0.8.4->ray[rllib]==0.8.4) (2.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from beautifulsoup4->google->ray==0.8.4->ray[rllib]==0.8.4) (2.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from packaging->tensorboardX->ray[rllib]==0.8.4) (2.4.7)\n",
      "Requirement already satisfied: opencv-python in /home/supremusdominus/.local/lib/python3.7/site-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from opencv-python) (1.21.0)\n",
      "Requirement already satisfied: opencv-python-headless==4.1.2.30 in /home/supremusdominus/.local/lib/python3.7/site-packages (4.1.2.30)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from opencv-python-headless==4.1.2.30) (1.21.0)\n"
     ]
    }
   ],
   "source": [
    "# # We install these specific versions of tensorflow and rllib, that we used in our work.\n",
    "!pip install gym==0.21\n",
    "!pip install tensorflow==1.14\n",
    "!pip install \"ray[rllib]==0.8.4\"\n",
    "!pip install opencv-python\n",
    "!pip install opencv-python-headless==4.1.2.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.14.0\n",
      "  Downloading https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.14.0-py3-none-any.whl (105.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.8/105.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from tensorflow==1.14.0) (2.0.0)\n",
      "Collecting astor>=0.6.0 (from tensorflow==1.14.0)\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: gast>=0.2.0 in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from tensorflow==1.14.0) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from tensorflow==1.14.0) (0.2.0)\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow==1.14.0)\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow==1.14.0)\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from tensorflow==1.14.0) (1.22.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/supremusdominus/.local/lib/python3.8/site-packages (from tensorflow==1.14.0) (1.16.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from tensorflow==1.14.0) (3.19.0)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow==1.14.0)\n",
      "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow==1.14.0)\n",
      "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.5/488.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from tensorflow==1.14.0) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/supremusdominus/.local/lib/python3.8/site-packages (from tensorflow==1.14.0) (1.12.1)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from tensorflow==1.14.0) (1.59.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from tensorflow==1.14.0) (0.38.4)\n",
      "Requirement already satisfied: h5py in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (3.10.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/supremusdominus/.local/lib/python3.8/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (67.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/supremusdominus/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (6.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from werkzeug>=0.11.15->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.17.0)\n",
      "Installing collected packages: tensorflow-estimator, keras-preprocessing, astor, keras-applications, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.13.0\n",
      "    Uninstalling tensorflow-estimator-2.13.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.13.0\n",
      "    Uninstalling tensorboard-2.13.0:\n",
      "      Successfully uninstalled tensorboard-2.13.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.13.1\n",
      "    Uninstalling tensorflow-2.13.1:\n",
      "      Successfully uninstalled tensorflow-2.13.1\n",
      "Successfully installed astor-0.8.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n",
      "Requirement already satisfied: rlib in /home/supremusdominus/anaconda3/envs/Dominus/lib/python3.8/site-packages (0.0.2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !pip install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.14.0-py3-none-any.whl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T21:06:34.489829Z",
     "start_time": "2023-05-02T21:06:34.331623Z"
    },
    "id": "V_aEzYs7rQbs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rl-warp-drive\n",
      "  Using cached rl_warp_drive-2.5.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gym>=0.26 (from rl-warp-drive)\n",
      "  Using cached gym-0.26.2.tar.gz (721 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from rl-warp-drive) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from rl-warp-drive) (1.21.0)\n",
      "Collecting pycuda>=2022.1 (from rl-warp-drive)\n",
      "  Using cached pycuda-2022.1-cp37-cp37m-linux_x86_64.whl\n",
      "Requirement already satisfied: pytest>=6.1.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from rl-warp-drive) (6.2.4)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /home/supremusdominus/.local/lib/python3.7/site-packages (from rl-warp-drive) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.9 in /home/supremusdominus/.local/lib/python3.7/site-packages (from rl-warp-drive) (1.13.1)\n",
      "Collecting numba>=0.54.0 (from rl-warp-drive)\n",
      "  Using cached numba-0.56.4-cp37-cp37m-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from gym>=0.26->rl-warp-drive) (2.2.1)\n",
      "Collecting gym-notices>=0.0.4 (from gym>=0.26->rl-warp-drive)\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from gym>=0.26->rl-warp-drive) (6.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/supremusdominus/.local/lib/python3.7/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/supremusdominus/.local/lib/python3.7/site-packages (from matplotlib>=3.2.1->rl-warp-drive) (2.8.1)\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0 (from numba>=0.54.0->rl-warp-drive)\n",
      "  Using cached llvmlite-0.39.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "Requirement already satisfied: setuptools in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (from numba>=0.54.0->rl-warp-drive) (68.2.2)\n",
      "Collecting pytools>=2011.2 (from pycuda>=2022.1->rl-warp-drive)\n",
      "  Using cached pytools-2022.1.12-py2.py3-none-any.whl\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from pycuda>=2022.1->rl-warp-drive) (1.4.4)\n",
      "Collecting mako (from pycuda>=2022.1->rl-warp-drive)\n",
      "  Using cached Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/supremusdominus/.local/lib/python3.7/site-packages (from pytest>=6.1.0->rl-warp-drive) (21.2.0)\n",
      "Requirement already satisfied: iniconfig in /home/supremusdominus/.local/lib/python3.7/site-packages (from pytest>=6.1.0->rl-warp-drive) (1.1.1)\n",
      "Requirement already satisfied: packaging in /home/supremusdominus/.local/lib/python3.7/site-packages (from pytest>=6.1.0->rl-warp-drive) (20.9)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in /home/supremusdominus/.local/lib/python3.7/site-packages (from pytest>=6.1.0->rl-warp-drive) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/supremusdominus/.local/lib/python3.7/site-packages (from pytest>=6.1.0->rl-warp-drive) (1.10.0)\n",
      "Requirement already satisfied: toml in /home/supremusdominus/.local/lib/python3.7/site-packages (from pytest>=6.1.0->rl-warp-drive) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (from torch>=1.9->rl-warp-drive) (3.10.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/supremusdominus/.local/lib/python3.7/site-packages (from torch>=1.9->rl-warp-drive) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/supremusdominus/.local/lib/python3.7/site-packages (from torch>=1.9->rl-warp-drive) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/supremusdominus/.local/lib/python3.7/site-packages (from torch>=1.9->rl-warp-drive) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/supremusdominus/.local/lib/python3.7/site-packages (from torch>=1.9->rl-warp-drive) (11.7.99)\n",
      "Requirement already satisfied: wheel in /home/supremusdominus/.local/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->rl-warp-drive) (0.38.4)\n",
      "Requirement already satisfied: six in /home/supremusdominus/.local/lib/python3.7/site-packages (from cycler>=0.10->matplotlib>=3.2.1->rl-warp-drive) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/supremusdominus/.local/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gym>=0.26->rl-warp-drive) (3.15.0)\n",
      "Collecting platformdirs>=2.2.0 (from pytools>=2011.2->pycuda>=2022.1->rl-warp-drive)\n",
      "  Using cached platformdirs-3.11.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions (from torch>=1.9->rl-warp-drive)\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/supremusdominus/anaconda3/envs/myenv/lib/python3.7/site-packages (from mako->pycuda>=2022.1->rl-warp-drive) (2.1.3)\n",
      "Using cached rl_warp_drive-2.5.0-py3-none-any.whl (390 kB)\n",
      "Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Using cached platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827622 sha256=456462b4d650e86a7f00050caddb79a58386f7c699aa26c92a5d6b1d947f3685\n",
      "  Stored in directory: /home/supremusdominus/.cache/pip/wheels/4d/c8/dc/d08577bffa680f083f04448e81e4f176d0e45a13eb16f93c0c\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, typing-extensions, llvmlite, platformdirs, pytools, numba, mako, gym, pycuda, rl-warp-drive\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "  Attempting uninstall: gym\n",
      "    Found existing installation: gym 0.21.0\n",
      "    Uninstalling gym-0.21.0:\n",
      "      Successfully uninstalled gym-0.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ai-economist 1.7.1 requires MarkupSafe==2.0.1, but you have markupsafe 2.1.3 which is incompatible.\n",
      "ai-economist 1.7.1 requires typing-extensions==3.10.0.0, but you have typing-extensions 4.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gym-0.26.2 gym-notices-0.0.8 llvmlite-0.39.1 mako-1.2.4 numba-0.56.4 platformdirs-3.11.0 pycuda-2022.1 pytools-2022.1.12 rl-warp-drive-2.5.0 typing-extensions-4.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rl-warp-drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T21:06:34.497798Z",
     "start_time": "2023-05-02T21:06:34.492800Z"
    },
    "id": "XXSxlt02KslN"
   },
   "outputs": [],
   "source": [
    "# Change directory to the tutorials folder\n",
    "import os, sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    os.chdir(\"/content/ai-economist/tutorials\")\n",
    "else:\n",
    "    os.chdir(os.path.dirname(os.path.abspath(\"__file__\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0ptmCDPKslN"
   },
   "source": [
    "## 1. Adding an Environment Wrapper "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56yP4q3zKslN"
   },
   "source": [
    "We first define a configuration (introduced in [the basics tutorial](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basics.ipynb)) for the \"gather-trade-build\" environment with multiple mobile agents (that move, gather resources, build or trade) and a social planner that sets taxes according to (a scaled variant of) the 2018 US tax schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T02:58:26.771229Z",
     "start_time": "2023-05-04T02:58:26.728867Z"
    },
    "id": "_YcfRA8NuAXN"
   },
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    # Scenario name - determines which scenario class to use\n",
    "    \"scenario_name\": \"CovidAndEconomySimulation\",\n",
    "    \n",
    "    # The list of components in this simulation\n",
    "    \"components\": [\n",
    "        {\"ControlUSStateOpenCloseStatus\": {\n",
    "            # action cooldown period in days.\n",
    "            # Once a stringency level is set, the state(s) cannot switch to another level\n",
    "            # for a certain number of days (referred to as the \"action_cooldown_period\")\n",
    "            \"action_cooldown_period\": 28\n",
    "        }},\n",
    "        {\"FederalGovernmentSubsidy\": {\n",
    "            # The number of subsidy levels.\n",
    "            \"num_subsidy_levels\": 20,\n",
    "            # The number of days over which the total subsidy amount is evenly rolled out.\n",
    "            \"subsidy_interval\": 90,\n",
    "            # The maximum annual subsidy that may be allocated per person.\n",
    "            \"max_annual_subsidy_per_person\": 20000,\n",
    "        }},\n",
    "        {\"VaccinationCampaign\": {\n",
    "            # The number of vaccines available per million people everyday.\n",
    "            \"daily_vaccines_per_million_people\": 3000,\n",
    "            # The number of days between vaccine deliveries.\n",
    "            \"delivery_interval\": 1,\n",
    "            # The date (YYYY-MM-DD) when vaccination begins\n",
    "            \"vaccine_delivery_start_date\": \"2021-01-12\",\n",
    "        }},\n",
    "    ],\n",
    "\n",
    "    # Date (YYYY-MM-DD) to start the simulation.\n",
    "    \"start_date\": \"2020-03-22\",\n",
    "    # How long to run the simulation for (in days)\n",
    "    \"episode_length\": 405,\n",
    "    \n",
    "    # use_real_world_data (bool): Replay what happened in the real world.\n",
    "    # Real-world data comprises SIR (susceptible/infected/recovered),\n",
    "    # unemployment, government policy, and vaccination numbers.\n",
    "    # This setting also sets use_real_world_policies=True.\n",
    "    \"use_real_world_data\": False,\n",
    "    # use_real_world_policies (bool): Run the environment with real-world policies\n",
    "    # (stringency levels and subsidies). With this setting and\n",
    "    # use_real_world_data=False, SIR and economy dynamics are still\n",
    "    # driven by fitted models.\n",
    "    \"use_real_world_policies\": False,\n",
    "    \n",
    "    # A factor indicating how much more the\n",
    "    # states prioritize health (roughly speaking, loss of lives due to\n",
    "    # opening up more) over the economy (roughly speaking, a loss in GDP\n",
    "    # due to shutting down resulting in more unemployment) compared to the\n",
    "    # real-world.\n",
    "    # For example, a value of 1 corresponds to the health weight that \n",
    "    # maximizes social welfare under the real-world policy, while\n",
    "    # a value of 2 means that states care twice as much about public health\n",
    "    # (preventing deaths), while a value of 0.5 means that states care twice\n",
    "    # as much about the economy (preventing GDP drops).\n",
    "    \"health_priority_scaling_agents\": 1,\n",
    "    # Same as above for the planner\n",
    "    \"health_priority_scaling_planner\": 1,\n",
    "    \n",
    "    # Full path to the directory containing\n",
    "    # the data, fitted parameters and model constants. This defaults to\n",
    "    # \"ai_economist/datasets/covid19_datasets/data_and_fitted_params\".\n",
    "    # For details on obtaining these parameters, please see the notebook\n",
    "    # \"ai-economist-foundation/ai_economist/datasets/covid19_datasets/\n",
    "    # gather_real_world_data_and_fit_parameters.ipynb\".\n",
    "    \"path_to_data_and_fitted_params\": \"\",\n",
    "    \n",
    "    # Economy-related parameters\n",
    "    # Fraction of people infected with COVID-19. Infected people don't work.\n",
    "    \"infection_too_sick_to_work_rate\": 0.1,\n",
    "    # Fraction of the population between ages 18-65.\n",
    "    # This is the subset of the population whose employment/unemployment affects\n",
    "    # economic productivity.\n",
    "    \"pop_between_age_18_65\": 0.6,\n",
    "    # Percentage of interest paid by the federal\n",
    "    # government to borrow money from the federal reserve for COVID-19 relief\n",
    "    # (direct payments). Higher interest rates mean that direct payments\n",
    "    # have a larger cost on the federal government's economic index.\n",
    "    \"risk_free_interest_rate\": 0.03,\n",
    "    # CRRA eta parameter for modeling the economic reward non-linearity.\n",
    "    \"economic_reward_crra_eta\": 2,\n",
    "       \n",
    "    # Number of agents in the simulation (50 US states + Washington DC)\n",
    "    \"n_agents\": 51,    \n",
    "    # World size: Not relevant to this simulation, but needs to be set for Foundation\n",
    "    \"world_size\": [1, 1],\n",
    "    # Flag to collate all the agents' observations, rewards and done flags into a single matrix\n",
    "    \"collate_agent_step_and_reset_data\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37_c6yeUKslO"
   },
   "source": [
    "Like we have seen in earlier [tutorials](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb), using `env = foundation.make_env_instance(**env_config)` creates an environment instance `env` with the specified configuration.\n",
    "\n",
    "In order to use this environment with RLlib, we will also need to add the environment's `observation_space` and `action_space` attributes. Additionally, the environment itself must subclass the [`MultiAgentEnv`](https://github.com/ray-project/ray/blob/master/rllib/env/multi_agent_env.py) interface, which can return observations and rewards from multiple ready agents per step. To this end, we use an environment [wrapper](https://github.com/salesforce/ai-economist/blob/master/tutorials/rllib/env_wrapper.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIve5a8XKslP"
   },
   "source": [
    "Upon applying the wrapper to our environment, we have now defined observation and action spaces for the agents and the planner, indicated with `(a)` and `(p)` respectively. Also, (a useful tip) you can still access the environment instance and its attributes simply by using `env_obj.env`\n",
    "\n",
    "In summary, the observation spaces are represented as `Box` objects and the action spaces as `Discrete` objects (for more details on these types, see the OpenAI documentation [page](https://gym.openai.com/docs/#spaces)).\n",
    "\n",
    "Briefly looking at the shapes of the observation features (the numbers in parentheses), you will see that we have some one-dimensional features (e.g. `action-mask`, `flat`, `time`) as well as spatial features (e.g., `world-idx-map`, `world-map`)\n",
    "\n",
    "A couple of quick notes:\n",
    "- An `action_mask` is used to mask out the actions that are not allowed by the environment. For instance, a mobile agent cannot move beyond the boundary of the world. Hence, in position (0, 0), a mobile cannot move \"Left\" or \"Down\", and the corresponding actions in the mask would be nulled out. Now, the RL agent can still recommend to move \"Left\" or \"Down\", but the action isn't really taken.\n",
    "- The key `flat` arises since we set `flatten_observations': True`. Accordingly, the scalar and vector raw observations are all concatenated into this single key. If you're curious to see the entire set of raw observations, do set `flatten_observations': False` in the env_config, and re-run the above cell.\n",
    "\n",
    "Looking at the action spaces, the mobile agents can take 50 possible actions (including 1 NO-OP action or do nothing (always indexed 0), 44 trading-related actions, 4 move actions along the four directions and 1 build action)\n",
    "\n",
    "The planner sets the tax rates for 7 brackets, each from 0-100% in steps of 5%, so that's 21 values. Adding the NO-OP action brings the planner action space to `MultiDiscrete([22 22 22 22 22 22 22])`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lIr7PJBKslP"
   },
   "source": [
    "## 2. Creating a *Trainer* Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bjMXlsHKslP"
   },
   "source": [
    "In order to train our economic simulation environment with RLlib, you will need familiarity with one of the key classes: the [`Trainer`](https://docs.ray.io/en/master/rllib-training.html). The trainer object maintains the relationships that connect each agent in the environment to its corresponding trainable policy, and essentially helps in training, checkpointing policies and inferring actions. It helps to co-ordinate the workflow of collecting rollouts and optimizing the various policies via a reinforcement learning algorithm. Inherently, RLlib maintains a wide suite of [algorithms](https://docs.ray.io/en/master/rllib-algorithms.html) for multi-agent learning (which was another strong reason for us to consider using RLlib) - available options include SAC, PPO, PG, A2C, A3C, IMPALA, ES, DDPG, DQN, MARWIL, APEX, and APEX_DDPG. For the remainder of this tutorial, we will stick to using [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/) (PPO), an algorithm known to perform well generally.\n",
    "\n",
    "Every algorithm has a corresponding trainer object; in the context of PPO, we invoke the `PPOTrainer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside covid19_components.py: 1 GPUs are available.\n",
      "Inside covid19_env.py: 1 GPUs are available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/supremusdominus/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/supremusdominus/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/supremusdominus/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/supremusdominus/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/supremusdominus/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/supremusdominus/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/supremusdominus/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/supremusdominus/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/supremusdominus/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/supremusdominus/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/supremusdominus/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/supremusdominus/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the activation code already present in '/home/supremusdominus/Download/ai-economist/ai_economist/foundation/activation_code.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 00:32:27,934\tINFO resource_spec.py:212 -- Starting Ray with 1.03 GiB memory available for workers and up to 0.53 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the real-world data to only initialize the env, and using the fitted models to step through the env.\n",
      "Loading real-world data from /home/supremusdominus/Download/ai-economist/ai_economist/foundation/scenarios/covid19/../../../datasets/covid19_datasets/data_and_fitted_params\n",
      "Loading fit parameters from /home/supremusdominus/Download/ai-economist/ai_economist/foundation/scenarios/covid19/../../../datasets/covid19_datasets/data_and_fitted_params\n",
      "Using external action inputs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 00:32:28,478\tINFO services.py:1148 -- View the Ray dashboard at \u001b[1m\u001b[32m127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2023-10-31 00:32:28,797\tINFO trainer.py:428 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
      "2023-10-31 00:32:29,186\tWARNING logger.py:308 -- Could not instantiate TBXLogger: module 'torch' has no attribute 'Tensor'.\n",
      "2023-10-31 00:32:29,206\tINFO trainer.py:585 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the activation code already present in '/home/supremusdominus/Download/ai-economist/ai_economist/foundation/activation_code.txt'\n",
      "Using the real-world data to only initialize the env, and using the fitted models to step through the env.\n",
      "Loading real-world data from /home/supremusdominus/Download/ai-economist/ai_economist/foundation/scenarios/covid19/../../../datasets/covid19_datasets/data_and_fitted_params\n",
      "Loading fit parameters from /home/supremusdominus/Download/ai-economist/ai_economist/foundation/scenarios/covid19/../../../datasets/covid19_datasets/data_and_fitted_params\n",
      "Using external action inputs.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env_config_dict = {\n",
    "    # Scenario name - determines which scenario class to use\n",
    "    \"scenario_name\": \"CovidAndEconomySimulation\",\n",
    "    \n",
    "    # The list of components in this simulation\n",
    "    \"components\": [\n",
    "        {\"ControlUSStateOpenCloseStatus\": {\n",
    "            # action cooldown period in days.\n",
    "            # Once a stringency level is set, the state(s) cannot switch to another level\n",
    "            # for a certain number of days (referred to as the \"action_cooldown_period\")\n",
    "            \"action_cooldown_period\": 28\n",
    "        }},\n",
    "        {\"FederalGovernmentSubsidy\": {\n",
    "            # The number of subsidy levels.\n",
    "            \"num_subsidy_levels\": 20,\n",
    "            # The number of days over which the total subsidy amount is evenly rolled out.\n",
    "            \"subsidy_interval\": 90,\n",
    "            # The maximum annual subsidy that may be allocated per person.\n",
    "            \"max_annual_subsidy_per_person\": 20000,\n",
    "        }},\n",
    "        {\"VaccinationCampaign\": {\n",
    "            # The number of vaccines available per million people everyday.\n",
    "            \"daily_vaccines_per_million_people\": 3000,\n",
    "            # The number of days between vaccine deliveries.\n",
    "            \"delivery_interval\": 1,\n",
    "            # The date (YYYY-MM-DD) when vaccination begins\n",
    "            \"vaccine_delivery_start_date\": \"2021-01-12\",\n",
    "        }},\n",
    "    ],\n",
    "\n",
    "    # Date (YYYY-MM-DD) to start the simulation.\n",
    "    \"start_date\": \"2020-03-22\",\n",
    "    # How long to run the simulation for (in days)\n",
    "    \"episode_length\": 405,\n",
    "    \n",
    "    # use_real_world_data (bool): Replay what happened in the real world.\n",
    "    # Real-world data comprises SIR (susceptible/infected/recovered),\n",
    "    # unemployment, government policy, and vaccination numbers.\n",
    "    # This setting also sets use_real_world_policies=True.\n",
    "    \"use_real_world_data\": False,\n",
    "    # use_real_world_policies (bool): Run the environment with real-world policies\n",
    "    # (stringency levels and subsidies). With this setting and\n",
    "    # use_real_world_data=False, SIR and economy dynamics are still\n",
    "    # driven by fitted models.\n",
    "    \"use_real_world_policies\": False,\n",
    "    \n",
    "    # A factor indicating how much more the\n",
    "    # states prioritize health (roughly speaking, loss of lives due to\n",
    "    # opening up more) over the economy (roughly speaking, a loss in GDP\n",
    "    # due to shutting down resulting in more unemployment) compared to the\n",
    "    # real-world.\n",
    "    # For example, a value of 1 corresponds to the health weight that \n",
    "    # maximizes social welfare under the real-world policy, while\n",
    "    # a value of 2 means that states care twice as much about public health\n",
    "    # (preventing deaths), while a value of 0.5 means that states care twice\n",
    "    # as much about the economy (preventing GDP drops).\n",
    "    \"health_priority_scaling_agents\": 1,\n",
    "    # Same as above for the planner\n",
    "    \"health_priority_scaling_planner\": 1,\n",
    "    \n",
    "    # Full path to the directory containing\n",
    "    # the data, fitted parameters and model constants. This defaults to\n",
    "    # \"ai_economist/datasets/covid19_datasets/data_and_fitted_params\".\n",
    "    # For details on obtaining these parameters, please see the notebook\n",
    "    # \"ai-economist-foundation/ai_economist/datasets/covid19_datasets/\n",
    "    # gather_real_world_data_and_fit_parameters.ipynb\".\n",
    "    \"path_to_data_and_fitted_params\": \"\",\n",
    "    \n",
    "    # Economy-related parameters\n",
    "    # Fraction of people infected with COVID-19. Infected people don't work.\n",
    "    \"infection_too_sick_to_work_rate\": 0.1,\n",
    "    # Fraction of the population between ages 18-65.\n",
    "    # This is the subset of the population whose employment/unemployment affects\n",
    "    # economic productivity.\n",
    "    \"pop_between_age_18_65\": 0.6,\n",
    "    # Percentage of interest paid by the federal\n",
    "    # government to borrow money from the federal reserve for COVID-19 relief\n",
    "    # (direct payments). Higher interest rates mean that direct payments\n",
    "    # have a larger cost on the federal government's economic index.\n",
    "    \"risk_free_interest_rate\": 0.03,\n",
    "    # CRRA eta parameter for modeling the economic reward non-linearity.\n",
    "    \"economic_reward_crra_eta\": 2,\n",
    "       \n",
    "    # Number of agents in the simulation (50 US states + Washington DC)\n",
    "    \"n_agents\": 51,    \n",
    "    # World size: Not relevant to this simulation, but needs to be set for Foundation\n",
    "    \"world_size\": [1, 1],\n",
    "    # Flag to collate all the agents' observations, rewards and done flags into a single matrix\n",
    "    \"collate_agent_step_and_reset_data\": True,\n",
    "}\n",
    "from rllib.env_wrapper import RLlibEnvWrapper\n",
    "env_obj = RLlibEnvWrapper({\"env_config_dict\": env_config_dict})\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "\n",
    "policies = {\n",
    "    \"a\": (\n",
    "        None,  # uses default policy\n",
    "        env_obj.observation_space,\n",
    "        env_obj.action_space,\n",
    "        {}  # define a custom agent policy configuration.\n",
    "    ),\n",
    "    \"p\": (\n",
    "        None,  # uses default policy\n",
    "        env_obj.observation_space,\n",
    "        env_obj.action_space_pl,\n",
    "        {}  # define a custom planner policy configuration.\n",
    "    )\n",
    "}\n",
    "\n",
    "# In foundation, all the agents have integer ids and the social planner has an id of \"p\"\n",
    "policy_mapping_fun = lambda i: \"a\" if str(i).isdigit() else \"p\"\n",
    "\n",
    "policies_to_train = [\"a\", \"p\"]\n",
    "\n",
    "trainer_config = {\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policies,\n",
    "        \"policies_to_train\": policies_to_train,\n",
    "        \"policy_mapping_fn\": policy_mapping_fun,\n",
    "    }\n",
    "}\n",
    "\n",
    "trainer_config.update(\n",
    "    {\n",
    "        \"num_workers\": 0,\n",
    "        \"num_envs_per_worker\": 0,\n",
    "        # Other training parameters\n",
    "        # \"train_batch_size\":  4000,\n",
    "        # \"sgd_minibatch_size\": 4000,\n",
    "        # \"num_sgd_iter\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "# We also add the \"num_envs_per_worker\" parameter for the env. wrapper to index the environments.\n",
    "env_config = {\n",
    "    \"env_config_dict\": env_config_dict,\n",
    "    \"num_envs_per_worker\": trainer_config.get('num_envs_per_worker'),\n",
    "}\n",
    "\n",
    "trainer_config.update(\n",
    "    {\n",
    "        \"env_config\": env_config\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(webui_host=\"127.0.0.1\")\n",
    "\n",
    "# Create the PPO trainer.\n",
    "trainer = PPOTrainer(\n",
    "    env=RLlibEnvWrapper,\n",
    "    config=trainer_config,\n",
    ")\n",
    "\n",
    "NUM_ITERS = 100 \n",
    "for iteration in range(NUM_ITERS):\n",
    "    print(f'********** Iter : {iteration} **********')\n",
    "    result = trainer.train()\n",
    "    print(f'''episode_reward_mean: {result.get('episode_reward_mean')}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv8bKQIkKslP"
   },
   "source": [
    "PPOTrainer can be instantiated with \n",
    "- `env`: an environment creator (i.e, RLlibEnvWrapper() in our case)\n",
    "- `config`: algorithm-specific configuration data for setting the various components of the RL training loop including the environment, rollout worker processes, training resources, degree of parallelism, framework used, and the policy exploration strategies.\n",
    "\n",
    "Note: There are several configuration settings related to policy architectures, rollout collection, minibatching, and other important hyperparameters, that need to be set carefully in order to train effectively. For the sake of the high-level exposition, we allow RLlib to use most of the the default settings. Check out the list of default [common configuration parameters](https://docs.ray.io/en/releases-0.8.4/rllib-training.html#common-parameters) and default [PPO-specific configuration parameters](https://docs.ray.io/en/releases-0.8.4/rllib-algorithms.html?highlight=PPO#proximal-policy-optimization-ppo). Custom environment configurations may be passed to environment creator via `config[\"env_config\"]`.\n",
    "\n",
    "RLlib also chooses default built-in [models](https://docs.ray.io/en/releases-0.8.4/rllib-models.html#built-in-models-and-preprocessors) for processing the observations. The models are picked based on a simple heuristic: a [vision](https://github.com/ray-project/ray/blob/master/rllib/models/tf/visionnet.py) network for observations that have shape of length larger than 2 (for example, (84 x 84 x 3)), and a [fully connected](https://github.com/ray-project/ray/blob/master/rllib/models/tf/fcnet.py) network for everything else. Custom models can be configured via the `config[\"policy\"][\"model\"]` key.\n",
    "\n",
    "In the context of multi-agent training, we will also need to set the multi-agent configuration:\n",
    "```python\n",
    "\"multiagent\": {\n",
    "        # Map of type MultiAgentPolicyConfigDict from policy ids to tuples\n",
    "        # of (policy_cls, obs_space, act_space, config). This defines the\n",
    "        # observation and action spaces of the policies and any extra config.\n",
    "        \"policies\": {},\n",
    "        # Function mapping agent ids to policy ids.\n",
    "        \"policy_mapping_fn\": None,\n",
    "        # Optional list of policies to train, or None for all policies.\n",
    "        \"policies_to_train\": None,\n",
    "    },\n",
    "```\n",
    "\n",
    "To this end, let's notate the agent policy id by `\"a\"` and the planner policy id by `\"p\"`. We can set `policies`, `policy_mapping_fun` and `policies_to_train` as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To2aBqVkKslQ"
   },
   "source": [
    "Create a multiagent trainer config holding the trainable policies and their mappings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlXDzKXVKslQ"
   },
   "source": [
    "With distributed RL, architectures typically comprise several **roll-out** and **trainer** workers operating in tandem\n",
    "![](assets/distributed_rl_architecture.png)\n",
    "\n",
    "The roll-out workers repeatedly step through the environment to generate and collect roll-outs in parallel, using the actions sampled from the policy models on the roll-out workers or provided by the trainer worker.\n",
    "Roll-out workers typically use CPU machines, and sometimes, GPU machines for richer environments.\n",
    "Trainer workers gather the roll-out data (asynchronously) from the roll-out workers and optimize policies on CPU or GPU machines.\n",
    "\n",
    "In this context, we can also add a `num_workers` configuration parameter to specify the number of rollout workers, i.e, those responsible for gathering rollouts. Note: setting `num_workers=0` will mean the rollouts will be collected by the trainer worker itself. Also, each worker can collect rollouts from multiple environments in parallel, which is specified in `num_envs_per_worker`; there will be a total of `num_workers` $\\times$ `num_envs_per_worker` environment replicas used to gather rollouts.\n",
    "Note: below, we also update some of the default trainer settings to keep the iteration time small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ymw2yiAQKslR"
   },
   "source": [
    "Finally, we need to add the environment configuration to the trainer configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTToecbKKslR"
   },
   "source": [
    "One the training configuration is set, we will need to initialize ray and create the PPOTrainer object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvsQg49FKslR"
   },
   "source": [
    "## 3. Perform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmCNTKcDKslR"
   },
   "source": [
    "And that's it! We are now ready to perform training by invoking `trainer.train()`; we call it for just a few number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Rosuv0AtKslR",
    "outputId": "63a068b5-fb3a-4a8a-f111-f176374fe632",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Iter : 0 **********\n"
     ]
    },
    {
     "ename": "RayTaskError(AssertionError)",
     "evalue": "\u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=15697, ip=172.23.26.13)\n  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 407, in ray._raylet.execute_task.function_executor\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 492, in sample\n    batches = [self.input_reader.next()]\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 53, in next\n    batches = [self.get_data()]\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 96, in get_data\n    item = next(self.rollout_provider)\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 346, in _env_runner\n    callbacks, soft_horizon, no_done_at_end)\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 441, in _process_observations\n    policy_id).transform(raw_obs)\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/models/preprocessors.py\", line 232, in transform\n    self.write(observation, array, 0)\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/models/preprocessors.py\", line 240, in write\n    (len(observation), len(self.preprocessors))\nAssertionError: (4, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(AssertionError)\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7762d82f5b60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_ITERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'********** Iter : {iteration} **********'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'''episode_reward_mean: {result.get('episode_reward_mean')}'''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m                         \u001b[0;34m\"continue training without the failed worker, set \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m                         \"`'ignore_worker_failures': True`.\")\n\u001b[0;32m--> 502\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# allow logs messages to propagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMAX_WORKER_FAILURE_RETRIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRayError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ignore_worker_failures\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \"\"\"\n\u001b[1;32m    260\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_train() needs to return a dict.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mafter_optimizer_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0mafter_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/ray/rllib/optimizers/multi_gpu_optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_fragment_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs_per_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                                           self.train_batch_size)\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                     logger.info(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/ray/rllib/optimizers/rollout.py\u001b[0m in \u001b[0;36mcollect_samples\u001b[0;34m(agents, rollout_fragment_length, num_envs_per_worker, train_batch_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mfut_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mnext_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray_get_and_free\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfut_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mnum_timesteps_so_far\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnext_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtrajectories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/ray/rllib/utils/memory.py\u001b[0m in \u001b[0;36mray_get_and_free\u001b[0;34m(object_ids)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_to_free\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mobject_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobject_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_ids, timeout)\u001b[0m\n\u001b[1;32m   1511\u001b[0m                     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_object_store_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayTaskError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayTaskError(AssertionError)\u001b[0m: \u001b[36mray::RolloutWorker.sample()\u001b[39m (pid=15697, ip=172.23.26.13)\n  File \"python/ray/_raylet.pyx\", line 452, in ray._raylet.execute_task\n  File \"python/ray/_raylet.pyx\", line 407, in ray._raylet.execute_task.function_executor\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 492, in sample\n    batches = [self.input_reader.next()]\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 53, in next\n    batches = [self.get_data()]\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 96, in get_data\n    item = next(self.rollout_provider)\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 346, in _env_runner\n    callbacks, soft_horizon, no_done_at_end)\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 441, in _process_observations\n    policy_id).transform(raw_obs)\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/models/preprocessors.py\", line 232, in transform\n    self.write(observation, array, 0)\n  File \"/home/supremusdominus/.local/lib/python3.7/site-packages/ray/rllib/models/preprocessors.py\", line 240, in write\n    (len(observation), len(self.preprocessors))\nAssertionError: (4, 5)"
     ]
    }
   ],
   "source": [
    "NUM_ITERS = 100 \n",
    "for iteration in range(NUM_ITERS):\n",
    "    print(f'********** Iter : {iteration} **********')\n",
    "    result = trainer.train()\n",
    "    print(f'''episode_reward_mean: {result.get('episode_reward_mean')}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HctwigPSKslR"
   },
   "source": [
    "By default, the results will be logged to a subdirectory of `~/ray_results`. This subdirectory will contain a file `params.json` which contains the hyperparameters, a file `result.json` which contains a training summary for each episode and a TensorBoard file that can be used to visualize training process with TensorBoard by running|\n",
    "```shell\n",
    "tensorboard --logdir ~/ray_results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YInHZMtYKslS"
   },
   "source": [
    "## 4. Generate and Visualize the Environment's Dense Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8hDNhyeKslS"
   },
   "source": [
    "At any point during training, we would also want to inspect the environment's dense logs in order to deep-dive into the training results. Introduced in our [basic tutorial](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb#Visualize-using-dense-logging), dense logs are basically logs of each agent's states, actions and rewards at every point in time, along with a snapshot of the world state.\n",
    "\n",
    "There are two equivalent ways to fetch the environment's dense logs using the trainer object.\n",
    "\n",
    "a. Simply retrieve the dense log from the workers' environment objects\n",
    "\n",
    "b. Generate dense log(s) from the most recent trainer policy model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7nf_Hy0KslS"
   },
   "source": [
    "### 4a. Simply retrieve the dense log from the workers' environment objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbYDzVkvKslS"
   },
   "source": [
    "From each rollout worker, it's straightforward to retrieve the dense logs using some of the function attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oOPkYoToKslS"
   },
   "outputs": [],
   "source": [
    "# Below, we fetch the dense logs for each rollout worker and environment within\n",
    "\n",
    "dense_logs = {}\n",
    "# Note: worker 0 is reserved for the trainer actor\n",
    "for worker in range((trainer_config[\"num_workers\"] > 0), trainer_config[\"num_workers\"] + 1):\n",
    "    for env_id in range(trainer_config[\"num_envs_per_worker\"]):\n",
    "        dense_logs[\"worker={};env_id={}\".format(worker, env_id)] = \\\n",
    "        trainer.workers.foreach_worker(lambda w: w.async_env)[worker].envs[env_id].env.previous_episode_dense_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy5bGmvPKslS"
   },
   "outputs": [],
   "source": [
    "# We should have num_workers x num_envs_per_worker number of dense logs\n",
    "print(dense_logs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zm0S6QneKslS"
   },
   "source": [
    "### 4b. Generate a dense log from the most recent trainer policy model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbILmlVrKslS"
   },
   "source": [
    "We may also use the trainer object directly to play out an episode. The advantage of this approach is that we can re-sample the policy model any number of times and generate several rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2oIv-StKslS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from utils import plotting  # plotting utilities for visualizing env. state\n",
    "\n",
    "def do_plot(env, ax, fig):\n",
    "    \"\"\"Plots world state during episode sampling.\"\"\"\n",
    "    maps = env.env.world.maps\n",
    "    locs = [agent.loc for agent in env.env.world.agents] \n",
    "\n",
    "    cmap_order = None\n",
    "    plotting.plot_map(maps, locs, ax, cmap_order)\n",
    "    ax.set_aspect('equal')\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "def generate_rollout_from_current_trainer_policy(\n",
    "    trainer, \n",
    "    env_obj,\n",
    "    num_dense_logs=1\n",
    "):\n",
    "    dense_logs = {}\n",
    "    for idx in range(num_dense_logs):\n",
    "        # Set initial states\n",
    "        agent_states = {}\n",
    "        for agent_idx in range(env_obj.env.n_agents):\n",
    "            agent_states[str(agent_idx)] = trainer.get_policy(\"a\").get_initial_state()\n",
    "        planner_states = trainer.get_policy(\"p\").get_initial_state()   \n",
    "\n",
    "        # Play out the episode\n",
    "        obs = env_obj.reset(force_dense_logging=True)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        for t in range(env_obj.env.episode_length):\n",
    "            actions = {}\n",
    "            for agent_idx in range(env_obj.env.n_agents):\n",
    "                # Use the trainer object directly to sample actions for each agent\n",
    "                actions[str(agent_idx)] = trainer.compute_action(\n",
    "                    obs[str(agent_idx)], \n",
    "                    agent_states[str(agent_idx)], \n",
    "                    policy_id=\"a\",\n",
    "                    full_fetch=False\n",
    "                )\n",
    "\n",
    "            # Action sampling for the planner\n",
    "            actions[\"p\"] = trainer.compute_action(\n",
    "                obs['p'], \n",
    "                planner_states, \n",
    "                policy_id='p',\n",
    "                full_fetch=False\n",
    "            )\n",
    "\n",
    "            obs, rew, done, info = env_obj.step(actions)        \n",
    "            if ((t+1) % 100) == 0:\n",
    "                do_plot(env_obj, ax, fig)\n",
    "            if done['__all__']:\n",
    "                break\n",
    "        dense_logs[idx] = env_obj.env.dense_log\n",
    "    return dense_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wj10lcDVKslT"
   },
   "outputs": [],
   "source": [
    "dense_logs = generate_rollout_from_current_trainer_policy(\n",
    "    trainer, \n",
    "    env_obj,\n",
    "    num_dense_logs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRfC7ap8KslT"
   },
   "source": [
    "### Visualizing the episode dense logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJiAqD5NKslT"
   },
   "source": [
    "Once we obtain the dense logs, we can use the plotting utilities we have created to examine the episode dense logs and visualize the the world state, agent-wise quantities, movement, and trading events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLbq2m3jKslT"
   },
   "outputs": [],
   "source": [
    "from utils import plotting  # plotting utilities for visualizing env. state\n",
    "\n",
    "dense_log_idx = 0\n",
    "for index, element in enumerate(dense_logs):\n",
    "  print('Plot ', index)\n",
    "  plotting.breakdown(dense_logs[index]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CgYa-IlKslT"
   },
   "outputs": [],
   "source": [
    "# Shutdown Ray after use\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzYAX5FYKslT"
   },
   "source": [
    "And that's it for now. See you in the [next](https://github.com/salesforce/ai-economist/blob/master/tutorials/two_level_curriculum_learning_with_rllib.md) tutorial :)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "9dbfbadc71e42abc9cd558c210cdff0e47003e6a6f6a3b108a42ec1174a9608f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
